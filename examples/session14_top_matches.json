{
  "session": 14,
  "date": "2026-02-08",
  "total_high_confidence_matches": 20,
  "top_20_matches": [
    {
      "similarity": 0.937,
      "mechanism": "scaling",
      "domains": "cs \u2194 cond-mat",
      "titles": [
        "Inverse Depth Scaling From Most Layers Being Similar",
        "Broken neural scaling laws in materials science"
      ],
      "descriptions": [
        "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requirin",
        "We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law tha"
      ],
      "arxiv_ids": [
        "2602.05970v1",
        "2602.05702v1"
      ]
    },
    {
      "similarity": 0.932,
      "mechanism": "scaling",
      "domains": "cs \u2194 cond-mat",
      "titles": [
        "Inverse Depth Scaling From Most Layers Being Similar",
        "Broken neural scaling laws in materials science"
      ],
      "descriptions": [
        "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requirin",
        "Neural scaling laws provide a framework for quantifying this behavior and guide the design of materials datasets and machine learning architectures"
      ],
      "arxiv_ids": [
        "2602.05970v1",
        "2602.05702v1"
      ]
    },
    {
      "similarity": 0.926,
      "mechanism": "network_effect",
      "domains": "cond-mat \u2194 q-bio",
      "titles": [
        "DMFlow: Disordered Materials Generation by Flow Matching",
        "MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network"
      ],
      "descriptions": [
        "The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme",
        "Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric "
      ],
      "arxiv_ids": [
        "2602.04734v1",
        "2602.01751v2"
      ]
    },
    {
      "similarity": 0.925,
      "mechanism": "network_effect",
      "domains": "stat \u2194 q-bio",
      "titles": [
        "CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs",
        "MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network"
      ],
      "descriptions": [
        "While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to impro",
        "Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric "
      ],
      "arxiv_ids": [
        "2602.05861v1",
        "2602.01751v2"
      ]
    },
    {
      "similarity": 0.921,
      "mechanism": "network_effect",
      "domains": "stat \u2194 cond-mat",
      "titles": [
        "CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs",
        "DMFlow: Disordered Materials Generation by Flow Matching"
      ],
      "descriptions": [
        "While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to impro",
        "The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme"
      ],
      "arxiv_ids": [
        "2602.05861v1",
        "2602.04734v1"
      ]
    },
    {
      "similarity": 0.825,
      "mechanism": "fine_tuning",
      "domains": "cs \u2194 q-bio",
      "titles": [
        "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds",
        "ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design"
      ],
      "descriptions": [
        "919 without additional fine-tuning",
        "081 after fine-tuning to 0"
      ],
      "arxiv_ids": [
        "2602.05557v1",
        "2602.00157v2"
      ]
    },
    {
      "similarity": 0.815,
      "mechanism": "scaling",
      "domains": "cond-mat \u2194 physics",
      "titles": [
        "Broken neural scaling laws in materials science",
        "Unified MPI Parallelization of Wave Function Methods: iCIPT2 as a Showcase"
      ],
      "descriptions": [
        "We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law tha",
        "It is also shown that the error of iCIPT2 follows a power law with respect to the number of configuration state functions"
      ],
      "arxiv_ids": [
        "2602.05702v1",
        "2602.04470v1"
      ]
    },
    {
      "similarity": 0.8,
      "mechanism": "fine_tuning",
      "domains": "cs \u2194 q-bio",
      "titles": [
        "Can vision language models learn intuitive physics from interaction?",
        "ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design"
      ],
      "descriptions": [
        "Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks",
        "081 after fine-tuning to 0"
      ],
      "arxiv_ids": [
        "2602.06033v1",
        "2602.00157v2"
      ]
    },
    {
      "similarity": 0.797,
      "mechanism": "fine_tuning",
      "domains": "biology \u2194 cs",
      "titles": [
        "Systematic review of self-supervised foundation models for brain network representation using electr",
        "Can vision language models learn intuitive physics from interaction?"
      ],
      "descriptions": [
        "Downstream tasks varied widely and implemented diverse fine-tuning strategies, which made direct comparison challenging",
        "Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks"
      ],
      "arxiv_ids": [
        "2602.03269v1",
        "2602.06033v1"
      ]
    },
    {
      "similarity": 0.793,
      "mechanism": "scaling",
      "domains": "stat \u2194 cond-mat",
      "titles": [
        "Optimal scaling laws in learning hierarchical multi-index models",
        "Broken neural scaling laws in materials science"
      ],
      "descriptions": [
        "In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a gen",
        "Neural scaling laws provide a framework for quantifying this behavior and guide the design of materials datasets and machine learning architectures"
      ],
      "arxiv_ids": [
        "2602.05846v1",
        "2602.05702v1"
      ]
    },
    {
      "similarity": 0.787,
      "mechanism": "fine_tuning",
      "domains": "biology \u2194 q-bio",
      "titles": [
        "Systematic review of self-supervised foundation models for brain network representation using electr",
        "ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design"
      ],
      "descriptions": [
        "Downstream tasks varied widely and implemented diverse fine-tuning strategies, which made direct comparison challenging",
        "081 after fine-tuning to 0"
      ],
      "arxiv_ids": [
        "2602.03269v1",
        "2602.00157v2"
      ]
    },
    {
      "similarity": 0.787,
      "mechanism": "fine_tuning",
      "domains": "biology \u2194 cs",
      "titles": [
        "Systematic review of self-supervised foundation models for brain network representation using electr",
        "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds"
      ],
      "descriptions": [
        "Downstream tasks varied widely and implemented diverse fine-tuning strategies, which made direct comparison challenging",
        "919 without additional fine-tuning"
      ],
      "arxiv_ids": [
        "2602.03269v1",
        "2602.05557v1"
      ]
    },
    {
      "similarity": 0.786,
      "mechanism": "combinatorial",
      "domains": "cs \u2194 physics",
      "titles": [
        "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
        "Branch-and-Bound Tensor Networks for Exact Ground-State Characterization"
      ],
      "descriptions": [
        "These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization pro",
        "Characterizing the ground-state properties of disordered systems, such as spin glasses and combinatorial optimization problems, is fundamental to scie"
      ],
      "arxiv_ids": [
        "2602.05920v1",
        "2602.05470v1"
      ]
    },
    {
      "similarity": 0.786,
      "mechanism": "optimization",
      "domains": "cs \u2194 physics",
      "titles": [
        "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
        "Branch-and-Bound Tensor Networks for Exact Ground-State Characterization"
      ],
      "descriptions": [
        "These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization pro",
        "Characterizing the ground-state properties of disordered systems, such as spin glasses and combinatorial optimization problems, is fundamental to scie"
      ],
      "arxiv_ids": [
        "2602.05920v1",
        "2602.05470v1"
      ]
    },
    {
      "similarity": 0.782,
      "mechanism": "adaptation",
      "domains": "biology \u2194 cs",
      "titles": [
        "FOVI: A biologically-inspired foveated interface for deep vision models",
        "Layer-wise LoRA fine-tuning: a similarity metric approach"
      ],
      "descriptions": [
        "We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, le",
        "Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the p"
      ],
      "arxiv_ids": [
        "2602.03766v1",
        "2602.05988v1"
      ]
    },
    {
      "similarity": 0.78,
      "mechanism": "fine_tuning",
      "domains": "cs \u2194 q-bio",
      "titles": [
        "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection",
        "ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design"
      ],
      "descriptions": [
        "We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised f",
        "081 after fine-tuning to 0"
      ],
      "arxiv_ids": [
        "2602.05874v1",
        "2602.00157v2"
      ]
    },
    {
      "similarity": 0.779,
      "mechanism": "scaling",
      "domains": "cs \u2194 q-bio",
      "titles": [
        "Inverse Depth Scaling From Most Layers Being Similar",
        "Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Represent"
      ],
      "descriptions": [
        "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requirin",
        "However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is "
      ],
      "arxiv_ids": [
        "2602.05970v1",
        "2601.22757v1"
      ]
    },
    {
      "similarity": 0.779,
      "mechanism": "adaptation",
      "domains": "cs \u2194 biology",
      "titles": [
        "Shared LoRA Subspaces for almost Strict Continual Learning",
        "FOVI: A biologically-inspired foveated interface for deep vision models"
      ],
      "descriptions": [
        "While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learn",
        "We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, le"
      ],
      "arxiv_ids": [
        "2602.06043v1",
        "2602.03766v1"
      ]
    },
    {
      "similarity": 0.779,
      "mechanism": "scaling",
      "domains": "cond-mat \u2194 physics",
      "titles": [
        "Broken neural scaling laws in materials science",
        "Was Benoit Mandelbrot a hedgehog or a fox?"
      ],
      "descriptions": [
        "We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law tha",
        "Across his diverse pursuits, the concept of scaling -- manifested in self-similarity, power laws, fractals, and multifractals -- served as the central"
      ],
      "arxiv_ids": [
        "2602.05702v1",
        "2602.01122v1"
      ]
    },
    {
      "similarity": 0.779,
      "mechanism": "scaling",
      "domains": "cond-mat \u2194 physics",
      "titles": [
        "Broken neural scaling laws in materials science",
        "Was Benoit Mandelbrot a hedgehog or a fox?"
      ],
      "descriptions": [
        "We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law tha",
        "Across his diverse pursuits, the concept of scaling -- manifested in self-similarity, power laws, fractals, and multifractals -- served as the central"
      ],
      "arxiv_ids": [
        "2602.05702v1",
        "2602.01122v1"
      ]
    }
  ],
  "notes": [
    "V2 algorithm with min_similarity=0.6 (raised from 0.5)",
    "All patterns normalized to canonical mechanisms (0% NULL)",
    "536 high-confidence matches (\u22650.7) - up from 135!",
    "Top mechanisms: bound (139), complexity (62), equilibrium (45), scaling (40)"
  ]
}