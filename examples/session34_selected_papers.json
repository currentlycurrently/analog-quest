[
  {
    "id": 448,
    "title": "Intermittent precipitation and spatial Allee effects drive irregular vegetation patterns in semiarid ecosystems",
    "abstract": "Vegetation in semi-arid ecosystems frequently organizes into spatially heterogeneous mosaics that regulate ecosystem functioning, productivity, and resilience. These patterns arise from local biological interactions, including facilitation among neighboring plants and competition for limiting resources. Classical theoretical approaches have attributed such organization to scale-dependent feedbacks, predicting regular spatial patterns and abrupt transitions to collapse. However, growing empirical and theoretical evidence reveal that environmental variability and demographic stochasticity can fundamentally reshape spatial organization, driving irregular clusters, dynamic mosaics, and gradual rather than catastrophic vegetation declines. In drylands, rainfall variability is a dominant source of environmental forcing: precipitation typically occurs in short, irregular pulses that transiently enhance survival and recruitment before competitive interactions again dominate. Near persistence thresholds, ecosystem dynamics are therefore governed not only by average climatic conditions but also by the timing and spatial coincidence of favorable events. Under these conditions, positive density dependence and local facilitation can critically determine whether vegetation patches persist, expand, or collapse. Here, we develop an individual-based model that integrates intermittent precipitation with local Allee effects to examine how stochastic rainfall shapes spatial organization and persistence. We show that the interaction between pulsed resource availability and density-dependent survival generates irregular cluster structures and strongly modulates extinction risk, with resilience emerging from local spatial covariance and neighborhood density rather than from total biomass alone. These results highlight the importance of individual-level, stochastic processes in determining ecosystem resilience.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.05583v1"
  },
  {
    "id": 449,
    "title": "Specieslike clusters based on identical ancestor points",
    "abstract": "We introduce several axioms which may or may not hold for any given subgraph of the directed graph of all organisms (past, present and future) where edges represent biological parenthood, with the simplifying background assumption that life does not go extinct. We argue these axioms are plausible for species: if one were to define species based purely on genealogical relationships, it would be reasonable to define them in such a way as to satisfy these axioms. The main axiom we introduce, which we call the identical ancestor point axiom, states that for any organism in any species, either the species contains at most finitely many descendants of that organism, or else the species contains at most finitely many non-descendants of that organism. We show that this (together with a convexity axiom) reduces the subjectivity of species, in a technical sense. We call connected sets satisfying these two axioms \"specieslike clusters.\" We consider the question of identifying a set of biologically plausible constraints that would guarantee every organism inhabits a maximal specieslike cluster subject to those constraints. We provide one such set consisting of two constraints and show that no proper subset thereof suffices.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.05274v1"
  },
  {
    "id": 450,
    "title": "Combination therapy for colorectal cancer with anti-PD-L1 and cancer vaccine: A multiscale mathematical model of tumor-immune interactions",
    "abstract": "The tumor-immune system plays a critical role in colorectal cancer progression. Recent preclinical and clinical studies showed that combination therapy with anti-PD-L1 and cancer vaccines improved treatment response. In this study, we developed a multiscale mathematical model of interactions among tumors, immune cells, and cytokines to investigate tumor evolutionary dynamics under different therapeutic strategies. Additionally, we established a computational framework based on approximate Bayesian computation to generate virtual tumor samples and capture inter-individual heterogeneity in treatment response. The results demonstrated that a multiple low-dose regimen significantly reduced advanced tumor burden compared to baseline treatment in anti-PD-L1 therapy. In contrast, the maximum dose therapy yielded superior tumor growth control in cancer vaccine therapy. Furthermore, cytotoxic T cells were identified as a consistent predictive biomarker both before and after treatment initiation. Notably, the cytotoxic T cells-to-regulatory T cells ratio specifically served as a robust pre-treatment predictive biomarker, offering potential clinical utility for patient stratification and therapy personalization.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.05118v1"
  },
  {
    "id": 451,
    "title": "Optimal Harvesting in Stream Networks: Maximizing Biomass and Yield",
    "abstract": "In this study, we develop a metapopulation model framework to identify optimal harvesting strategies for a population in a stream network. We consider two distinct optimization objectives: maximization of total biomass and maximization of total yield, under the constraint of a fixed total harvesting effort. We examine in detail the special case of a two-patch network and fully characterize the optimal strategies for each objective. We show that when the population growth rate exceeds a critical threshold, a single harvesting strategy can simultaneously maximize both objectives. For general $n$-patch networks with homogeneous growth rates across patches, we focus on the regime of large growth rates and demonstrate that the optimal harvesting strategy selects patches according to their intraspecific competition rates and an effective net flow metric determined by network connectivity parameters.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.05071v1"
  },
  {
    "id": 452,
    "title": "Uncertainty in Island-based Ecosystem Services and Climate Change",
    "abstract": "Small and medium-sized islands are acutely exposed to climate change and ecosystem degradation, yet the extent to which uncertainty is systematically addressed in scientific assessments of their ecosystem services remains poorly understood. This study revisits 226 peer-reviewed articles drawn from two global systematic reviews on island ecosystem services and climate change, applying a structured post hoc analysis to evaluate how uncertainty is treated across methods, service categories, ecosystem realms, and decision contexts. Studies were classified according to whether uncertainty was explicitly analysed, just mentioned, or ignored. Only 30 percent of studies incorporated uncertainty explicitly, while more than half did not address it at all. Scenario-based approaches dominated uncertainty assessment, whereas probabilistic and ensemble-based frameworks remained limited. Cultural ecosystem services and extreme climate impacts exhibited the lowest levels of uncertainty integration, and few studies connected uncertainty treatment to policy relevant decision frameworks. Weak or absent treatment of uncertainty emerges as a structural challenge in island systems, where narrow ecological thresholds, strong land-sea coupling, limited spatial buffers, and reduced institutional redundancy amplify the consequences of decision-making under incomplete knowledge. Systematic mapping of how uncertainty is framed, operationalised, or neglected reveals persistent methodological and conceptual gaps and informs concrete directions for strengthening uncertainty integration in future island-focused ecosystem service and climate assessments. Embedding uncertainty more robustly into modelling practices, participatory processes, and policy tools is essential for enhancing scientific credibility, governance relevance, and adaptive capacity in insular socio-ecological systems.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.04762v1"
  },
  {
    "id": 453,
    "title": "How seed banks evolve in plants: a stochastic dynamical system subject to a strong drift",
    "abstract": "We study how changes in population size and fluctuating environmental conditions influence the establishment of seed banks in plants. Our model is a modification of the Wright-Fisher model with seed bank, introduced by Kaj, Krone and Lascoux. We distinguish between wild type individuals, producing only nondormant seeds, and mutants, producing seeds with dormancy. To understand how changing population size shapes the establishment of seed banks, we analyse the process under a diffusive scaling. The results support the biological insight that seed banks are favoured in a declining population, and disfavoured if population size is constant or increasing. The surprise is that this is true even when population sizes are changing very slowly -- over evolutionary timescales. We also investigate the influence of short-term fluctuations, such as annual variations in rainfall or temperature. Mathematically, our analysis reduces to a stochastic dynamical system forced onto a manifold by a large drift, which converges under scaling to a diffusion on the manifold. Inspired by the Lyapunov--Schmidt reduction, we derive an explicit formula for the limiting diffusion coefficients by projecting the system onto its linear counterpart. This provides a general framework for deriving diffusion approximations in models with strong drift and nonlinear constraints.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.04437v1"
  },
  {
    "id": 454,
    "title": "A brief review of evolutionary game dynamics in the reinforcement learning paradigm",
    "abstract": "Cooperation, fairness, trust, and resource coordination are cornerstones of modern civilization, yet their emergence remains inadequately explained by the persistent discrepancies between theoretical predictions and behavioral experiments. Part of this gap may arise from the imitation learning paradigm commonly used in prior theoretical models, which assumes individuals merely copy successful neighbors according to predetermined, fixed rules. This review examines recent advances in evolutionary game dynamics that employ reinforcement learning (RL) as an alternative paradigm. In RL, individuals learn through trial and error and introspectively refine their strategies based on environmental feedback. We begin by introducing key concepts in evolutionary game theory and the two learning paradigms, then synthesize progress in applying RL to elucidate cooperation, trust, fairness, optimal resource coordination, and ecological dynamics. Collectively, these studies indicate that RL offers a promising unified framework for understanding the diverse social and ecological phenomena observed in human and natural systems.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.04150v1"
  },
  {
    "id": 455,
    "title": "Deep-learning-based pan-phenomic data reveals the explosive evolution of avian visual disparity",
    "abstract": "The evolution of biological morphology is critical for understanding the diversity of the natural world, yet traditional analyses often involve subjective biases in the selection and coding of morphological traits. This study employs deep learning techniques, utilising a ResNet34 model capable of recognising over 10,000 bird species, to explore avian morphological evolution. We extract weights from the model's final fully connected (fc) layer and investigate the semantic alignment between the high-dimensional embedding space learned by the model and biological phenotypes. The results demonstrate that the high-dimensional embedding space encodes phenotypic convergence. Subsequently, we assess the morphological disparity among various taxa and evaluate the association between morphological disparity and species richness, demonstrating that species richness is the primary driver of morphospace expansion. Moreover, the disparity-through-time analysis reveals a visual \"early burst\" after the K-Pg extinction.\n  While mainly aimed at evolutionary analysis, this study also provides insights into the interpretability of Deep Neural Networks. We demonstrate that hierarchical semantic structures (biological taxonomy) emerged in the high-dimensional embedding space despite being trained on flat labels. Furthermore, through adversarial examples, we provide evidence that our model in this task can overcome texture bias and learn holistic shape representations (body plans), challenging the prevailing view that CNNs rely primarily on local textures.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.03824v1"
  },
  {
    "id": 456,
    "title": "Asymptotic Behavior of Integral Projection Models via Genealogical Quantities",
    "abstract": "Multi-state structured population models, including integral projection models (IPMs) and age-structured McKendrick equations, link individual life histories to population growth and composition, yet the demographic meaning of their dominant eigenstructure can be difficult to interpret. A main goal of this paper is to derive interpretable demographic indicators for multi-state heterogeneity -- in particular expected generation numbers, which act as an effective genealogical memory length (in generations) of the ancestry-weighted contributions driving growth -- together with type reproduction numbers and generation intervals, directly from life-history transition kernels.\n  To this end we develop a determinant-free genealogical framework based on a reference-point operator, a rank-one construction at the kernel level that singles out a biologically chosen reference state and organizes lineages by their contributions relative to that state. This yields stable distributions and reproductive values as convergent series of iterated kernels, and leads to an Euler--Lotka-like characteristic equation expressed by reference-point moments. The resulting expansion admits a closed combinatorial form via ordinary partial Bell polynomials, providing a direct bridge from transition kernels to genealogical quantities.\n  We extend the approach to multi-state McKendrick equations and show how these indicators quantify how population scale and composition are determined by ancestry-weighted initial-state information. The framework avoids restrictive Hilbert--Schmidt assumptions and clarifies how temporal memory and multi-type heterogeneity emerge from cross-generational accumulation, yielding a unified and interpretable route from transition kernels to multi-state demographic indicators.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.03228v1"
  },
  {
    "id": 457,
    "title": "From Discrete to Continuous Mixed Populations of Conformists, Nonconformists, and Imitators",
    "abstract": "In two-strategy decision-making problems, individuals often imitate the highest earners or choose either the common or rare strategy.\n  Individuals who benefit from the common strategy are conformists, whereas those who profit by choosing the less common one are called nonconformists.\n  The population proportions of the two strategies may undergo perpetual fluctuations\n  in finite, discrete, heterogeneous populations of imitators, conformists, and nonconformists.\n  How these fluctuations evolve as population size increases was left as an open question and is addressed in this paper.\n  We show that the family of Markov chains describing the discrete population dynamics forms a generalized stochastic approximation process for a differential inclusion--the continuous-time dynamics.\n  Furthermore, we prove that the continuous-time dynamics always equilibrate.\n  Then, by leveraging results from the stochastic approximation theory, we show that the amplitudes of fluctuations in the proportions of the two strategies in the population approach zero with probability one when the population size grows to infinity.\n  Our results suggest that large-scale perpetual fluctuations are unlikely in large, well-mixed populations consisting of these three types, particularly when imitators follow the highest earners.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.01492v1"
  },
  {
    "id": 458,
    "title": "Organismal Agency and Rapid Adaptation: The Phenopoiesis Algorithm for Phenotype-First Evolution",
    "abstract": "Evolutionary success depends on the capacity to adapt: organisms must respond to environmental challenges through both genetic innovation and lifetime learning. The gene-centric paradigm attributes evolutionary causality exclusively to genes, while Denis Noble's phenotype-first framework argues that organisms are active agents capable of interpreting genetic resources, learning from experience, and shaping their own development. However, this framework has remained philosophically intuitive but algorithmically opaque.\n  We show for the first time that organismal agency can be implemented as a concrete computational process through heritable phenotypic patterns. We introduce the Phenopoiesis Algorithm, where organisms inherit not just genes but also successful phenotypic patterns discovered during lifetime learning. Through experiments in changing environments, these pattern-inheriting organisms achieve 3.4 times faster adaptation compared to gene-centric models. Critically, these gains require cross-generational inheritance of learned patterns rather than within-lifetime learning alone.\n  We conclude that organismal agency is not a philosophical abstraction but an algorithmic mechanism with measurable adaptive value. The mechanism works through compositional reuse: organisms discover how to compose primitive elements into solutions, encode those compositional recipes, and transmit them to offspring. Evolution operates across multiple timescales -- fast, reversible phenotypic inheritance and slow, permanent genetic inheritance -- providing adaptive flexibility that single-channel mechanisms cannot achieve.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.00978v1"
  },
  {
    "id": 459,
    "title": "The Evolution of Lying in a Spatially-Explicit Prisoner's Dilemma Model",
    "abstract": "I present the results from a spatial model of the prisoner's dilemma, played on a toroidal lattice. Each individual has a default strategy of either cooperating ($C$) or defecting ($D$). Two strategies were tested, including ``tit-for-tat'' (TFT), in which individuals play their opponent's last play, or simply playing their default play. Each individual also has a probability of telling the truth ($0 \\leq P_{truth} \\leq 1$) about their last play. This parameter, which can evolve over time, allows individuals to be, for instance, a defector but present as a cooperator regarding their last play. This leads to interesting dynamics where mixed populations of defectors and cooperators with $P_{truth} \\geq 0.75$ move toward populations of truth-telling cooperators. Likewise, mixed populations with $P_{truth} < 0.7$ become populations of lying defectors. Both such populations are stable because they each have higher average scores than populations with intermediate values of $P_{truth}$. Applications of this model are discussed with regards to both humans and animals.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.02587v1"
  },
  {
    "id": 460,
    "title": "Microbiome association diversity reflects proximity to the edge of instability",
    "abstract": "Recent advances in metagenomics have revealed macroecological patterns or \"laws\" describing robust statistical regularities across microbial communities. Stochastic logistic models (SLMs), which treat species as independent -- akin to ideal gases in physics -- and incorporate environmental noise, reproduce many single-species patterns but cannot account for the pairwise covariation observed in microbiome data. Here we introduce an interacting stochastic logistic model (ISLM) that minimally extends the SLM by sampling an ensemble of random interaction networks chosen to preserve these single-species laws. Using dynamical mean-field theory, we map the model's phase diagram -- stable, chaotic, and unbounded-growth regimes -- where the transition from stable fixed-point to chaos is controlled by network sparsity and interaction heterogeneity via a May-like instability line. Going beyond mean-field theory to account for finite communities, we derive an estimator of an effective stability parameter that quantifies distance to the edge of instability and can be inferred from the width of the distribution of pairwise covariances in empirical species-abundance data. Applying this framework to synthetic data, environmental microbiomes, and human gut cohorts indicates that these communities tend to operate near the edge of instability. Moreover, gut communities from healthy individuals cluster closer to this edge and exhibit broader, more heterogeneous associations, whereas dysbiosis-associated states shift toward more stable regimes -- enabling discrimination across conditions such as Crohn's disease, inflammatory bowel syndrome, and colorectal cancer. Together, our results connect macroecological laws, interaction-network ensembles, and May's stability theory, suggesting that complex communities may benefit from operating near a dynamical phase transition.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.22918v1"
  },
  {
    "id": 461,
    "title": "Indirect Reciprocity with Environmental Feedback",
    "abstract": "Indirect reciprocity maintains cooperation in stranger societies by mapping individual behaviors onto reputation signals via social norms. Existing theoretical frameworks assume static environments with constant resources and fixed payoff structures. However, in real-world systems, individuals' strategic behaviors not only shape their reputation but also induce collective-level resource changes in ecological, economic, or other external environments, which in turn reshape the incentives governing future individual actions. To overcome this limitation, we establish a co-evolutionary framework that couples moral assessment, strategy updating, and environmental dynamics, allowing the payoff structure to dynamically adjust in response to the ecological consequences of collective actions. We find that this environmental feedback mechanism helps lower the threshold for the emergence of cooperation, enabling the system to spontaneously transition from a low-cooperation state to a stable high-cooperation regime, thereby reducing the dependence on specific initial conditions. Furthermore, while lenient norms demonstrate adaptability in static environments, norms with strict discrimination are shown to be crucial for curbing opportunism and maintaining evolutionary resilience in dynamic settings. Our results reveal the evolutionary dynamics of coupled systems involving reputation institutions and environmental constraints, offering a new theoretical perspective for understanding collective cooperation and social governance in complex environments.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.02553v1"
  },
  {
    "id": 462,
    "title": "Multi-strain SIS dynamics with coinfection under host population structure",
    "abstract": "Coinfection phenomena are common in nature, yet there is a lack of analytical approaches for coinfection systems with a high number of circulating and interacting strains. In this paper, we investigated a coinfection SIS framework applied to N strains, co-circulating in a structured host population. Adopting a general formulation for fixed host classes, defined by arbitrary epidemiological traits such as class-specific transmission rates, susceptibilities, clearance rates, etc., our model can be easily applied in different frameworks: for example, when different host species share the same pathogen, in classes of vaccinated or non-vaccinated hosts, or even in classes of hosts defined by the number of contacts. Using the strain similarity assumption, we identify the fast and slow variables of the epidemiological dynamics on the host population, linking neutral and non-neutral strain dynamics, and deriving a global replicator equation. This global replicator equation allows to explicitly predict coexistence dynamics from mutual invasibility coefficients among strains. The derived global pairwise invasion fitness matrix contains explicit traces of the underlying host population structure, and of its entanglement with the strain interaction and trait landscape. Our work thus enables a more comprehensive study and efficient simulation of multi-strain dynamics in endemic ecosystems, paving the way to deeper understanding of global persistence and selection forces, jointly shaped by pathogen and host diversity.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.00193v1"
  },
  {
    "id": 463,
    "title": "Epigenetic state inheritance drivers drug-tolerant persister-induced resistance in solid tumors: A stochastic agent-based model",
    "abstract": "The efficacy of anti-cancer therapies is severely limited by the emergence of drug resistance. While genetic drivers are well-characterized, growing evidence suggests that non-genetic mechanisms, particularly those involving drug-tolerant persisters (DTPs), play a pivotal role in solid tumor relapse. To elucidate the evolutionary dynamics of DTP-induced resistance, we develop a stochastic agent-based model (ABM) of solid tumor evolution that couples macroscopic population dynamics with microscopic epigenetic state inheritance during the cell cycle. Our simulations accurately reproduce the temporal progression of relapse observed in experimental studies, capturing the dynamic transition from sensitive cells to DTPs, and ultimately to stable resistant phenotypes under prolonged therapy. By explicitly modeling the epigenetic plasticity of individual cells, our model bridges the gap between cellular heterogeneity and population-level tumor evolution. Furthermore, we performed \\textit{in silico} clinical trials using virtual patient cohorts to evaluate therapeutic outcomes, demonstrating that optimized adaptive treatment strategies can significantly delay tumor relapse compared to standard dosing. This study provides a quantitative framework for dissecting DTP-driven resistance mechanisms and designing more effective, biologically informed therapeutic strategies.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.22619v1"
  },
  {
    "id": 464,
    "title": "Impact of behavioral heterogeneity on epidemic outcome and its mapping into effective network topologies",
    "abstract": "Human behavior plays a critical role in shaping epidemic trajectories. During health crises, people respond in diverse ways in terms of self-protection and adherence to recommended measures, largely reflecting differences in how individuals assess risk. This behavioral variability induces effective heterogeneity into key epidemic parameters, such as infectivity and susceptibility. We introduce a minimal extension of the susceptible-infected-removed~(SIR) model, denoted HeSIR, that captures these effects through a simple bimodal scheme, where individuals may have higher or lower transmission--related traits. We derive a closed-form expression for the epidemic threshold in terms of the model parameters, and the network's degree distribution and homophily, defined as the tendency of like--risk individuals to preferentially interact. We identify a resurgence regime just beyond the classical threshold, where the number of infected individuals may initially decline before surging into large-scale transmission. Through simulations on homogeneous and heterogeneous network topologies we corroborate the analytical results and highlight how variations in susceptibility and infectivity influence the epidemic dynamics. We further show that, under suitable assumptions, the HeSIR model maps onto a standard SIR process on an appropriately modified contact network, providing a unified interpretation in terms of structural connectivity. Our findings quantify the effect of heterogeneous behavioral responses, especially in the presence of homophily, and caution against underestimating epidemic potential in fragmented populations, which may undermine timely containment efforts. The results also extend to heterogeneity arising from biological or other non-behavioral sources.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.21743v1"
  },
  {
    "id": 465,
    "title": "Structural properties of distance-bounded phylogenetic reconciliation",
    "abstract": "Phylogenetic reconciliation seeks to explain host-symbiont co-evolution by mapping parasite trees onto host trees through events such as cospeciation, duplication, host switching, and loss. Finding an optimal reconciliation that ensures time feasibility is computationally hard when timing information is incomplete, and the complexity remains open when host switches are restricted by a fixed maximum distance $d$. While the case $d=2$ is known to be polynomial, larger values are unresolved. In this paper, we study the cases $d=3$ and $d=4$. We show that although arbitrarily large cycles may occur, it suffices to check only bounded-size cycles (we provide a complete list), provided the reconciliation satisfies acyclicity (i.e., time-feasibility) in a stronger sense. These results do not resolve the general complexity, but highlight structural properties that advance the understanding of distance-bounded reconciliations.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.22193v1"
  },
  {
    "id": 466,
    "title": "Long-term evolution of regulatory DNA sequences. Part 2: Theory and future challenges",
    "abstract": "Promoters and enhancers are cis-regulatory elements (CREs), DNA sequences that bind transcription factor (TF) proteins to up- or down-regulate target genes. Decades-long efforts yielded TF-DNA interaction models that predict how strongly an individual TF binds arbitrary DNA sequences and how individual binding events on the CRE combine to affect gene expression. These insights can be synthesized into a global, biophysically-realistic, and quantitative genotype-phenotype (GP) map for gene regulation, a \"holy grail\" for the application of evolutionary theory. A global map provides a rare opportunity to simulate long-term evolution of regulatory sequences and pose several fundamental questions: How long does it take to evolve CREs de novo? How many non-trivial regulatory functions exist in sequence space? How connected are they? For which regulatory architecture is CRE evolution most rapid and evolvable? In this article, the second of a two-part series, we review the application of evolutionary concepts - epistasis, robustness, evolvability, tunability, plasticity, and bet-hedging - to the evolution of gene regulatory sequences. We then evaluate the potential for a unifying theory for the evolution of regulatory sequences, and identify key open challenges.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.21480v1"
  },
  {
    "id": 467,
    "title": "The quenched structured coalescent for diploid population models on finite graphs with large migrations and uneven offspring distributions",
    "abstract": "In this work we describe a new model for the evolution of a diploid structured population backwards in time that allows for large migrations and uneven offspring distributions. The model generalizes both the mean-field model of Birkner et al. [\\textit{Electron. J. Probab.} 23: 1-44 (2018)] and the haploid structured model of M\u00f6hle [\\textit{Theor. Popul. Biol.} 2024 Apr:156:103-116]. We show convergence, with mild conditions on the joint distribution of offspring frequencies and migrations, of gene genealogies conditional on the pedigree to a time-inhomogeneous coalescent process driven by a Poisson point process $\u03a8$ that records the timing and scale of large migrations and uneven offspring distributions. This quenched scaling limit demonstrates a significant difference in the predictions of the classical annealed theory of structured coalescent processes. In particular, the annealed and quenched scaling limits coincide if and only if these large migrations and uneven offspring distributions are absent. The proof proceeds by the method of moments and utilizes coupling techniques from the theory of random walks in random environments. Several examples are given and their quenched scaling limits established.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.21079v1"
  },
  {
    "id": 468,
    "title": "Diversifying Toxicity Search in Large Language Models Through Speciation",
    "abstract": "Evolutionary prompt search is a practical black-box approach for red teaming large language models (LLMs), but existing methods often collapse onto a small family of high-performing prompts, limiting coverage of distinct failure modes. We present a speciated quality-diversity (QD) extension of ToxSearch that maintains multiple high-toxicity prompt niches in parallel rather than optimizing a single best prompt. ToxSearch-S introduces unsupervised prompt speciation via a search methodology that maintains capacity-limited species with exemplar leaders, a reserve pool for outliers and emerging niches, and species-aware parent selection that trades off within-niche exploitation and cross-niche exploration. ToxSearch-S is found to reach higher peak toxicity ($\\approx 0.73$ vs.\\ $\\approx 0.47$) and a extreme heavier tail (top-10 median $0.66$ vs.\\ $0.45$) than the baseline, while maintaining comparable performance on moderately toxic prompts. Speciation also yields broader semantic coverage under a topic-as-species analysis (higher effective topic diversity $N_1$ and larger unique topic coverage $K$). Finally, species formed are well-separated in embedding space (mean separation ratio $\\approx 1.93$) and exhibit distinct toxicity distributions, indicating that speciation partitions the adversarial space into behaviorally differentiated niches rather than superficial lexical variants. This suggests our approach uncovers a wider range of attack strategies.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.20981v1"
  },
  {
    "id": 1969,
    "title": "Growth Models Under Uniform Catastrophes",
    "abstract": "We consider stochastic growth models for populations organized in colonies and subject to uniform catastrophes. To assess population viability, we analyze scenarios in which individuals adopt dispersion strategies after catastrophic events. For these models, we derive explicit expressions for the survival probability and the mean time to extinction, both with and without spatial constraints. In addition, we complement this analysis by comparing uniform catastrophes with binomial and geometric catastrophes in models with dispersion and no spatial restrictions. Here, the terms uniform, binomial and geometric refer to the probability distributions governing the number of individuals that survive immediately after a catastrophe. This comparison allows us to quantify the impact of different types of catastrophic events on population persistence.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.06649v1"
  },
  {
    "id": 1970,
    "title": "Habitat heterogeneity and dispersal network structure as drivers of metacommunity dynamics",
    "abstract": "Spatial structure and species interactions jointly shape the dynamics and biodiversity of ecological systems, yet most theoretical models either neglect spatial heterogeneity or sacrifice analytical tractability. Here, we provide a unified microscopic, mechanistic framework for deriving effective metapopulation and metacommunity models from individual-based ecological dynamics on arbitrary dispersal networks. The resulting coarse-grained description features an effective dispersal kernel that encodes both microscopic dynamical parameters and network topology. Based on this framework, we demonstrate exact analytical results for species persistence in both homogeneous and heterogeneous landscapes, including a generalization of the classical concept of metapopulation capacity to non-uniform local extinction rates. Incorporating stochasticity arising from finite carrying capacities, we obtain a reduced one-dimensional description that reveals universal finite-size scaling laws for extinction times and fluctuations. Extending the approach to multiple competing species, we prove that in homogeneous environments monodominance can be avoided only in a fine-tuned, marginally stable coexistence state, and that the classic metapopulation capacity gives only a necessary but not sufficient condition for persistence. We demonstrate that heterogeneous habitats can support stable coexistence, but only above a critical level of heterogeneity. Finally, we outline how additional ecological processes can be systematically incorporated within the same formalism. Together, these results provide analytical benchmarks and a general route for constructing spatially explicit ecological theories based on an interpretable underlying mechanistic foundation.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.06640v1"
  },
  {
    "id": 1971,
    "title": "Multiple timescales in collective motion: daily and intraday upstream fish migration focusing on Feller condition",
    "abstract": "Fish migration is a collective phenomenon that has multiple timescales, ranging from daily to intraday (hourly or even finer). We propose a unified mathematical approach using diffusion bridges, nonlinear stochastic differential equations with pinned initial and terminal conditions, to model both daily and intraday fish migration phenomena. Drift and diffusion coefficients of these bridges are determined based on time-dependent parameterized average and variance curves fitted against fish count data, with which the unique existence of their solutions is rigorously guaranteed. We show that sample paths of the diffusion bridges have qualitatively distinctive properties depending on the Feller condition, namely, the ratio between the sizes of diffusion and drift. Our application study about the juvenile upstream migration of Plecoglossus altivelis altivelis (Ayu) in Japan clarifies similarities and differences between daily and intraday migration phenomena. Particularly, we discuss that the daily and intraday fish count data correspond to distinctive Feller indices, showing that the former is qualitatively less randomized and intermittent. The results obtained in this study suggest that the Feller condition potentially serves as an effective tool for evaluating fish migration phenomena of Ayu across different timescales.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.06606v1"
  },
  {
    "id": 1972,
    "title": "Threshold Resource Redistribution in Spatially-Structured Kinship Networks",
    "abstract": "We present a model for a threshold-based resource redistribution process in a spatially-explicit population, characterizing the relation between kinship network structure, local interactions and persistence. We find that population survival becomes possible for lower resource densities, but leads to increased network heterogeneity and locally centralized clusters. We interpret this in relation to a feedback between the kinship network structure and reproduction ability. Agents receive stochastic resources and solicit additional resources from connected individuals when below a minimum, with each agent contributing a fraction of their excess based on relatedness. We first analyze a fully-connected population with uniform redistribution fraction and discuss mean field expectations as well as finite size corrections. We extend this model to a hub-and-spoke network, exploring the impact of network asymmetry or centrality on resource distribution. We then develop a spatially-limited population model with diffusion, local pairing, reproduction and mortality. Redistribution is introduced as a function of relatedness (generational distance through most-recent common ancestor) and distance. Redistribution-dependent populations exhibit a higher level of relational closeness with increased clustering for agents of highest node strength. These results highlight the interaction of resource density, cooperation and kinship in a spatially-limited regime.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.06200v1"
  },
  {
    "id": 76,
    "title": "Trimming of extreme votes and favoritism: Evidence from the field",
    "abstract": "Despite a large body of theoretical literature on voting mechanisms, there is no documented evidence from real-world panel evaluations about the effect of trimming the extreme votes on sincere voting. We provide the first such evidence by comparing subjective evaluations of experts from different countries in competitive settings with and without a trimming mechanism. In these evaluations, some of the evaluated subjects are experts' compatriots. Using data on 29,383 subjective evaluations, we find that experts assign significantly higher scores to their compatriots in panels without trimming. However, in panels with trimming, this favoritism is generally insignificant.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.05542v1"
  },
  {
    "id": 77,
    "title": "Collaboration for the Bioeconomy -- Evidence from Innovation Output in Sweden, 1970-2021",
    "abstract": "Collaboration is expected to play a central role in the transition to a bioeconomy - a central pillar of a green economy. Such collaboration is supposed to connect traditional biomass processing firms with diverse actors in fields where biomass ought to substitute existing or create novel products and processes. This study analyzes the network of technology collaborations among innovating firms in Sweden between 1970 and 2021. The results reveal generally positive associations between direct and indirect ties, with meaningful increases in innovation output for each additional direct collaboration partner. Relationships between brokerage positions and innovation output were statistically insignificant, and cognitive proximity - while following theoretical expectations - materially insignificant. These associations are mostly equal between actors heavily invested in the bioeconomy and those focusing on other innovation areas, indicating that these actors operate under largely similar mechanisms linking collaboration and subsequent innovation output. These results suggest that stimulating collaboration broadly - rather than attempting to optimize collaboration compositions - could result in higher number of significant Swedish innovations, for bioeconomy and other sectors alike.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.05112v1"
  },
  {
    "id": 78,
    "title": "Discounted Sales of Expiring Perishables: Challenges for Demand Forecasting in Grocery Retail Practice",
    "abstract": "Grocery retailers frequently apply price discounts to stimulate demand for expiring perishables. However, integrating these discounted sales into future demand forecasts presents a significant challenge. This study investigates the effectiveness of incorporating a fixed share of these sales as \\textit{regular} demand into the forecast, as commonly applied in practice. We employ a two-step regression approach on data from a major European grocery retailer, covering over 1,700 products across 676 stores. We reveal that forecasts underestimate actual demand for most SKUs when discounted sales occur. This residual uplift effect is significantly influenced by the number of sales at reduced prices. Our findings underscore the necessity for more precise approaches to integrate discounted sales into demand forecasts, thereby preventing excess inventory and the associated economic and environmental impacts of spoilage in the grocery sector.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.04464v1"
  },
  {
    "id": 79,
    "title": "Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon",
    "abstract": "Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks. Here, we introduce such a framework that connects meteorology, AI, and social sciences. As an example, we apply it to the 150-year-old problem of Indian monsoon forecasting, focusing on benefits to rain-fed agriculture, which is highly susceptible to climate change. AIWP models skillfully predict an agriculturally relevant onset index at regional scales weeks in advance when evaluated out-of-sample using deterministic and probabilistic metrics. This framework informed a government-led effort in 2025 to send 38 million Indian farmers AI-based monsoon onset forecasts, which captured an unusual weeks-long pause in monsoon progression. This decision-oriented benchmarking framework provides a key component of a blueprint for harnessing the power of AIWP models to help large vulnerable populations adapt to weather shocks in the face of climate variability and change.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.03767v1"
  },
  {
    "id": 80,
    "title": "Tracing the Genetic Footprints of the UK National Health Service",
    "abstract": "The establishment of the UK National Health Service (NHS) in July 1948 was one of the most consequential health policy interventions of the twentieth century, providing universal and free access to medical care and substantially expanding maternal and infant health services. In this paper, we estimate the causal effect of the NHS introduction on early-life mortality and we test whether survival is selective. We adopt a regression discontinuity design under local randomization, comparing individuals born just before and just after July 1948. Leveraging newly digitized weekly death records, we document a significant decline in stillbirths and infant mortality following the introduction of the NHS, the latter driven primarily by reductions in deaths from congenital conditions and diarrhea. We then use polygenic indexes (PGIs), fixed at conception, to track changes in population composition, showing that cohorts born at or after the NHS introduction exhibit higher PGIs associated with contextually-adverse traits (e.g., depression, COPD, and preterm birth) and lower PGIs associated with contextually-valued traits (e.g., educational attainment, self-rated health, and pregnancy length), with effect sizes as large as 7.5% of a standard deviation. These results based on the UK Biobank data are robust to family-based designs and replicate in the English Longitudinal Study of Ageing and the UK Household Longitudinal Study. Effects are strongest in socioeconomically disadvantaged areas and among males. This novel evidence on the existence and magnitude of selective survival highlights how large-scale public policies can leave a persistent imprint on population composition and generate long-term survival biases.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.03751v1"
  },
  {
    "id": 81,
    "title": "Confrontation with the West and Long-Run Economic and Institutional Outcomes: Evidence from Iran",
    "abstract": "This paper studies the long-run economic and institutional consequences of Iran's confrontation with the West, treating the 2006-2007 strategic shift as the onset of a sustained confrontation regime rather than a discrete sanctions episode. Using synthetic control and generalized synthetic control methods, I construct transparent counterfactuals for Iran's post-confrontation trajectory from a donor pool of countries with continuously normalized relations with the West. I find large, persistent losses in real GDP and GDP per capita, accompanied by sharp declines in foreign direct investment, trade integration, and non-oil exports. These economic effects coincide with substantial and durable deterioration in political stability, rule of law, and control of corruption. Magnitude calculations imply cumulative output losses comparable to civil-war settings, despite the absence of internal armed conflict. The results highlight confrontation as a deep and persistent economic and institutional shock, extending the literature beyond short-run sanctions effects to sustained geopolitical isolation.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.03231v1"
  },
  {
    "id": 82,
    "title": "The long-run returns to breastfeeding",
    "abstract": "This paper shows that the mid-20th century was characterised by a considerable reduction in breastfeeding rates, reducing from over 80% in the late 1930s to just over 40% only three decades later. We investigate how maternal breastfeeding during this period has shaped offspring health and human capital outcomes in the UK. We use a within-family design, comparing children who were breastfed to their sibling(s) who were not. Our results show that breastfeeding increases adult height, as well as fluid intelligence, but does not affect educational attainment, nor adult BMI. In further analyses, we examine whether and how this impact varies with individuals' genetic \"predisposition\" for these outcomes, proxied by the outcome-specific polygenic index. We find that the \"height-returns\" to breastfeeding are larger among those genetically predisposed to be taller, with no genetic heterogeneity for the other outcomes, though we note that power in the within-family GxE analysis is more limited. Overall, our estimates suggest that breastfeeding plays a non-negligible role in child development.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.03221v1"
  },
  {
    "id": 83,
    "title": "Mathematical Modeling of Common-Pool Resources: A Comprehensive Review of Bioeconomics, Strategic Interaction, and Complex Adaptive Systems",
    "abstract": "The governance of common-pool resources-resource systems characterized by high subtractability of yield and difficulty of exclusion-constitutes one of the most persistent and intricate challenges in the fields of economics, ecology, and applied mathematics. This comprehensive review delineates the historical and theoretical evolution of the mathematical frameworks developed to analyze, predict, and manage these systems. We trace the intellectual trajectory from the early, deterministic bioeconomic models of the mid-20th century, which established the fundamental tension between individual profit maximization and collective efficiency, to the contemporary era of complex coupled human-environment system models. Our analysis systematically dissects the formalization of the \"Tragedy of the Commons\" through the lens of classical cooperative and non-cooperative game theory, examining how the N-person Prisoner's Dilemma and Nash Equilibrium concepts provided the initial, albeit pessimistic, predictive baseline. We subsequently explore the \"Ostrom Turn,\" which necessitated the integration of institutional realism-specifically monitoring, graduated sanctions, and communication-into formal game-theoretic structures. The review further investigates the relaxation of rationality assumptions via evolutionary game theory and behavioral economics, highlighting the destabilizing roles of prospect theory and hyperbolic discounting. Finally, we synthesize recent advances in stochastic differential equations and agent-based computational economics, which capture the critical roles of spatial heterogeneity, noise-induced regime shifts, and early warning signals of collapse. By unifying these diverse mathematical threads, this review elucidates the shifting paradigm from static optimization to dynamic resilience in the management of the commons.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.03129v1"
  },
  {
    "id": 84,
    "title": "Nota de Pol\u00edtica P\u00fablica: Quanto de produtividade precisamos para reduzir a jornada de trabalho?",
    "abstract": "This paper quantifies, within a short-run structural model with predetermined capital, the immediate effects of imposing a cap on formal working hours that reduces the weekly workweek from 44 to 36 hours. The central object is the total factor productivity required to preserve GDP at its baseline level, A_req, defined as the multiplicative factor applied to A_t that equates output under the policy to output in the baseline. In the baseline simulation, the 44 -> 36 transition implies A_req ~ 8.5%",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.03884v1"
  },
  {
    "id": 85,
    "title": "Applications of structural equation modeling and mathematical statistics to the triggering mechanism of a class of liquors consumer behaviors in Sichuan province",
    "abstract": "Structural Equation Modeling (SEM) systematically validated hierarchical pathways among multiple factors by constructing a dual framework integrating latent variable measurement and path analysis, utilizing covariance matrices derived from online questionnaires of Wuliangye consumers in Sichuan Province. Statistical analysis quantified path coefficient significance through maximum likelihood estimation, revealing via factor loadings and goodness-of-fit tests that consumer ethnocentrism directly promotes purchase intention, while simultaneously refuting the null hypothesis regarding perceived behavioral control-thus deconstructing the \"trigger-transmission\" causal chain among variables. Crucially, SEM findings revealed environmental stimuli as the predominant factor, indirectly influencing purchasing behavior through perceived value, contrary to existing literature asserting equal impacts from consumer ethnocentrism, environmental stimuli, and perceived behavioral control. Statistical evidence further demonstrated higher online purchase frequency for premium Wuliangye liquor, aligning with Generation Z's e-commerce preferences. By implementing stricter website-based participant screening than prior studies, this research optimized the analytical model, yielding data-driven strategic recommendations: strengthening e-commerce platforms, enhancing promotional expertise, leveraging cultural localization, and prioritizing premium product development. These actionable insights significantly advance sales optimization strategies for Wuliangye products in Sichuan's dynamic market.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.02956v1"
  },
  {
    "id": 86,
    "title": "Skill Substitution, Expectations, and the Business Cycle",
    "abstract": "This paper studies how labor market conditions around high school graduation affect postsecondary skill investments. Using administrative data on more than six million German graduates from 1995-2018, and exploiting deviations from secular state-specific trends, I document procyclical college enrollment. Cyclical increases in unemployment reduce enrollment at traditional universities and shift graduates toward vocational colleges and apprenticeships. These effects translate into educational attainment. Using large-scale survey data, I identify changes in expected returns to different degrees as the main mechanism. During recessions, graduates expect lower returns to an academic degree, while expected returns to a vocational degree are stable.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.02483v1"
  },
  {
    "id": 87,
    "title": "Strategic Interactions in Science and Technology Networks: Substitutes or Complements?",
    "abstract": "This paper develops a theory of scientific and technological peer effects to study how individuals' productivity responds to the behavior and network positions of their collaborators across both scientific and inventive activities. Building on a simultaneous equation network framework, the model predicts that productivity in each activity increases in a variation of the Katz-Bonacich centrality that captures within-activity and cross-activity strategic complementarities. To test these predictions, we assemble the universe of cancer-related publications and patents and construct coauthorship and coinventorship networks that jointly map the collaboration structure of researchers active in both spheres. Using an instrumental-variables approach based on predicted link formation from exogenous dyadic characteristics, and incorporating community fixed effects to address endogenous network formation, we show that both authors' and inventors' outputs rise with their network centrality, consistent with the theory. Moreover, scientific productivity significantly enhances technological productivity, while technological output does not exert a detectable reciprocal effect on scientific production, highlighting an asymmetric linkage aligned with a science-driven model of innovation. These findings provide the first empirical evidence on the joint dynamics of scientific and inventive peer effects, underscore the micro-foundations of the co-evolution of science and technology, and reveal how collaboration structures can be leveraged to design policies that enhance collective knowledge creation and downstream innovation.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.02403v1"
  },
  {
    "id": 88,
    "title": "Optimal Solar Investment and Operation under Asymmetric Net Metering",
    "abstract": "We examine the joint investment and operational decisions of a prosumer, a customer who both consumes and generates electricity, under net energy metering (NEM) tariffs. Traditional NEM schemes provide temporally flat compensation at the retail price for net energy exports over a billing period. However, ongoing reforms in several U.S. states are introducing time-varying prices and asymmetric import/export compensation to better align incentives with grid costs. While prior studies treat PV capacity as exogenous and focus primarily on consumption behavior, this work endogenizes PV investment and derives the marginal value of solar capacity for a flexible prosumer under asymmetric NEM tariffs. We characterize optimal investment and show how optimal investment changes with prices and PV costs. Through this analysis, we identify a PV effect: changes in NEM pricing in one period can influence net demand and consumption in generating periods with unchanged prices through adjustments in optimal PV investment. The PV effect weakens the ability of higher import prices to increase prosumer payments, with direct implications for NEM reform. We validate our theoretical results in a case study using simulated household and tariff data derived from historical conditions in Massachusetts.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.02284v1"
  },
  {
    "id": 89,
    "title": "The Strategic Foresight of LLMs: Evidence from a Fully Prospective Venture Tournament",
    "abstract": "Can artificial intelligence outperform humans at strategic foresight -- the capacity to form accurate judgments about uncertain, high-stakes outcomes before they unfold? We address this question through a fully prospective prediction tournament using live Kickstarter crowdfunding projects. Thirty U.S.-based technology ventures, launched after the training cutoffs of all models studied, were evaluated while fundraising remained in progress and outcomes were unknown. A diverse suite of frontier and open-weight large language models (LLMs) completed 870 pairwise comparisons, producing complete rankings of predicted fundraising success. We benchmarked these forecasts against 346 experienced managers recruited via Prolific and three MBA-trained investors working under monitored conditions. The results are striking: human evaluators achieved rank correlations with actual outcomes between 0.04 and 0.45, while several frontier LLMs exceeded 0.60, with the best (Gemini 2.5 Pro) reaching 0.74 -- correctly ordering nearly four of every five venture pairs. These differences persist across multiple performance metrics and robustness checks. Neither wisdom-of-the-crowd ensembles nor human-AI hybrid teams outperformed the best standalone model.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.01684v1"
  },
  {
    "id": 90,
    "title": "Hype Has Worth: Attention, Sentiment, and NFT Valuation in Major Ethereum Collections",
    "abstract": "Do online narratives leave a measurable imprint on prices in markets for digital or cultural goods? This paper evaluates how community attention and sentiment relate to valuation in major Ethereum NFT collections after accounting for time effects, market-wide conditions, and persistent visual heterogeneity. Transaction data for large generative collections are merged with Reddit-based discourse measures available for 25 collections, covering 87{,}696 secondary-market sales from January 2021 through March 2025. Visual differences are absorbed by a transparent, within-collection standardized index built from explicit image traits and aggregated via PCA. Discourse is summarized at the collection-by-bin level using discussion intensity and lexicon-based tone measures, with smoothing to reduce noise when text volume is sparse. A mixed-effects specification with a Mundlak within--between decomposition separates persistent cross-collection differences from within-collection fluctuations. Valuations align most strongly with sustained collection-level attention and sentiment environments; within collections, short-horizon negativity is consistently associated with higher prices, and attention is most informative when measured as cumulative engagement over multiple prior windows.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.01531v1"
  },
  {
    "id": 91,
    "title": "Legal Infrastructure for Transformative AI Governance",
    "abstract": "Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.01474v1"
  },
  {
    "id": 92,
    "title": "Calibrating Behavioral Parameters with Large Language Models",
    "abstract": "Behavioral parameters such as loss aversion, herding, and extrapolation are central to asset pricing models but remain difficult to measure reliably. We develop a framework that treats large language models (LLMs) as calibrated measurement instruments for behavioral parameters. Using four models and 24{,}000 agent--scenario pairs, we document systematic rationality bias in baseline LLM behavior, including attenuated loss aversion, weak herding, and near-zero disposition effects relative to human benchmarks. Profile-based calibration induces large, stable, and theoretically coherent shifts in several parameters, with calibrated loss aversion, herding, extrapolation, and anchoring reaching or exceeding benchmark magnitudes. To assess external validity, we embed calibrated parameters in an agent-based asset pricing model, where calibrated extrapolation generates short-horizon momentum and long-horizon reversal patterns consistent with empirical evidence. Our results establish measurement ranges, calibration functions, and explicit boundaries for eight canonical behavioral biases.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.01022v1"
  },
  {
    "id": 93,
    "title": "Electoral Polls and Economic Uncertainty: an Analysis of the Last Two U.S. Presidential Elections",
    "abstract": "This paper examines the dynamic relationship between electoral polls and indicators of economic and financial uncertainty during the last two U.S. presidential elections (2020 and 2024). Using daily polling data on Donald Trump and measures such as the Aruoba-Diebold-Scotti Business Conditions Index, the 5-year Breakeven Inflation Rate, the Trade Policy Uncertainty index, and the VIX, we estimate conditional correlation models to capture time-varying interactions. The analysis reveals that in 2020, correlations between polls and uncertainty measures were highly dynamic and event-driven, reflecting the influence of exogenous shocks (COVID-19, oil price collapse) and political milestones (primaries, debates). In contrast, during the 2024 campaign, correlations remained close to zero, stable, and largely unresponsive to shocks, suggesting that entrenched polarization and non-economic events (e.g., assassination attempt, candidate changes) muted the economic channel. The study highlights how the interplay between voter sentiment, financial markets, and uncertainty varies across electoral contexts, offering a methodological contribution through the application of Dynamic Conditional Correlation models to political data and policy-relevant insights on the conditions under which economic fundamentals influence electoral dynamics.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2601.21534v1"
  },
  {
    "id": 94,
    "title": "Payrolls to Prompts: Firm-Level Evidence on the Substitution of Labor for AI",
    "abstract": "Generative AI has the potential to transform how firms produce output. Yet, credible evidence on how AI is actually substituting for human labor remains limited. In this paper, we study firm-level substitution between contracted online labor and generative AI using payments data from a large U.S. expense management platform. We track quarterly spending from Q3 2021 to Q3 2025 on online labor marketplaces (such as Upwork and Fiverr) and leading AI model providers. To identify causal effects, we exploit the October 2022 release of ChatGPT as a common adoption shock and estimate a difference-in-differences model. We provide a novel measure of exposure based on the share of spending at online labor marketplaces prior to the shock. Firms with greater exposure to online labor adopt AI earlier and more intensively following the shock, while simultaneously reducing spending on contracted labor. By Q3 2025, firms in the highest exposure quartile increase their share of spending on AI model providers by 0.8 percentage points relative to the lowest exposure quartile, alongside significant declines in labor marketplace spending. Combining these responses yields a direct estimate of substitution: among the most exposed firms, a \\$1 decline in online labor spending is associated with approximately \\$0.03 of additional AI spending, implying order-of-magnitude cost savings from replacing outsourced tasks with AI services. These effects are heterogeneous across firms and emerge gradually over time. Taken together, our results provide the first direct, micro-level evidence that generative AI is being used as a partial substitute for human labor in production.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.00139v1"
  },
  {
    "id": 95,
    "title": "The Effects of Higher Education on Midlife Depression: Quasi-Experimental Evidence from South Korea",
    "abstract": "Higher education has expanded worldwide, with women outpacing men in many regions. While educational attainment is consistently linked to better physical health, its mental health effects - particularly for women - remain underexplored, and causal evidence is limited. We estimate the impact of college completion on depression among middle-aged women in South Korea, leveraging the 1993 higher education reform, which raised women's college attainment by 45 percentage points (pp) over the following decade. We use two nationally representative datasets to triangulate evidence, including the Korea National Health and Nutrition Examination Survey (KNHANES, 2007-2021) for physician-diagnosed depression, and the Korean Longitudinal Survey of Women and Families (KLoWF, 2007-2022) to validate findings using self-reports of depressive symptoms. We implement two-stage least squares (2SLS) with a birth-cohort instrument based on exposure to the reform (within 3 years of the cutoff in KNHANES and within 1 to 3 years in KLoWF). In KNHANES, college completion lowers physician-diagnosed depression by 2.4 pp, attenuating to 1.6 pp after adjusting for income, employment, and physical health. In KLoWF, college completion improves self-reported mental health. The weekly depressive-symptoms composite declines by 17.4 pp, attenuating to 16.4 pp after covariate adjustment. Placebo tests on unaffected cohorts yield null results. This study contributes to the growing quasi-experimental literature on education and mental health with convergent evidence across clinical diagnoses and self-reported depressive symptoms in South Korea. By focusing on college education in a non-Western setting, it extends the external validity of existing findings and highlights educational policy as a potential lever to reduce the burden of midlife depression among women.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2601.20976v1"
  },
  {
    "id": 96,
    "title": "A Smoothed GMM for Dynamic Quantile Preferences Estimation",
    "abstract": "This paper suggests methods for estimation of the $\u03c4$-quantile, $\u03c4\\in(0,1)$, as a parameter along with the other finite-dimensional parameters identified by general conditional quantile restrictions. We employ a generalized method of moments framework allowing for non-linearities and dependent data, where moment functions are smoothed to aid both computation and tractability. Consistency and asymptotic normality of the estimators are established under weak assumptions. Simulations illustrate the finite-sample properties of the methods. An empirical application using a quantile intertemporal consumption model with multiple assets estimates the risk attitude, which is captured by $\u03c4$, together with the elasticity of intertemporal substitution.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2601.20853v1"
  },
  {
    "id": 97,
    "title": "Regulatory Migration to Europe: ICO Reallocation Following U.S. Securities Enforcement",
    "abstract": "This paper examines whether a major U.S. regulatory clarification coincided with cross-border spillovers in crypto-asset entrepreneurial finance. We study the Securities and Exchange Commission's July 2017 DAO Report, which clarified the application of U.S. securities law to many initial coin offerings, and analyze how global issuance activity adjusted across regions. Using a comprehensive global dataset of ICOs from 2014 to 2021, we construct a region-month panel and evaluate issuance dynamics around the announcement. We document a substantial and persistent reallocation of ICO activity toward Europe following the DAO Report. In panel regressions with region and month fixed effects, Europe experiences an average post-2017 increase of approximately 14 additional ICOs per region-month relative to other regions, net of global market cycles. The results are consistent with cross-border regulatory spillovers in highly mobile digital-asset markets.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2602.00138v1"
  },
  {
    "id": 98,
    "title": "Clear Messages, Ambiguous Audiences: Measuring Interpretability in Political Communication",
    "abstract": "Text-based measurement in political research often treats classi6ication disagreement as random noise. We examine this assumption using con6idence-weighted human annotations of 5,000 social media messages by U.S. politicians. We 6ind that political communication is generally highly legible, with mean con6idence exceeding 0.99 across message type, partisan bias, and audience classi6ications. However, systematic variation concentrates in the constituency category, which exhibits a 1.79 percentage point penalty in audience classi6ication con6idence. Given the high baseline of agreement, this penalty represents a sharp relative increase in interpretive uncertainty. Within messages, intent remains clear while audience targeting becomes ambiguous. These patterns persist with politician 6ixed effects, suggesting that measurement error in political text is structured by strategic incentives rather than idiosyncratic coder error.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2601.20912v1"
  },
  {
    "id": 99,
    "title": "Pricing Catastrophe: How Extreme Political Shocks Reprice Sovereign Risk, Beliefs, and Growth Expectations",
    "abstract": "Extreme political shocks may reshape economies not only through contemporaneous disruption but by altering beliefs about the distribution of future states. We study how such belief ruptures affect the cost of capital, expectations, and macroeconomic dynamics, using the October 7, 2023 attack on Israel as a precisely timed shock. Leveraging monthly data from 2008 to 2025 and a donor pool of advanced economies, we estimate counterfactual paths using a matrix completion design with rolling-window cross-validation and placebo-based inference, corroborated by synthetic difference-in-differences. We document three core findings. First, long-horizon sovereign risk of Israel is persistently repriced. Ten-year yields and spreads relative to the United States rise sharply and remain elevated. Second, household welfare beliefs deteriorate durably, as reflected in consumer confidence. Third, medium-run momentum improves, captured by a strong rise in the OECD composite leading indicator. These patterns reveal risk-growth decoupling where tail-risk premia rise even as medium-horizon activity expectations strengthen. Our results highlight belief-driven channels as a central mechanism through which extreme ruptures shape macro-financial outcomes.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2601.20724v1"
  },
  {
    "id": 100,
    "title": "Normative Equivalence in Human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups",
    "abstract": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.",
    "domain": "econ",
    "subdomain": "econ.GN",
    "arxiv_id": "2601.20487v2"
  },
  {
    "id": 101,
    "title": "RareCollab -- An Agentic System Diagnosing Mendelian Disorders with Integrated Phenotypic and Molecular Evidence",
    "abstract": "Millions of children worldwide are affected by severe rare Mendelian disorders, yet exome and genome sequencing still fail to provide a definitive molecular diagnosis for a large fraction of patients, prolonging the diagnostic odyssey. Bridging this gap increasingly requires transitioning from DNA-only interpretation to multi-modal diagnostic reasoning that combines genomic data, transcriptomic sequencing (RNA-seq), and phenotype information; however, computational frameworks that coherently integrate these signals remain limited. Here we present RareCollab, an agentic diagnostic framework that pairs a stable quantitative Diagnostic Engine with Large Language Model (LLM)-based specialist modules that produce high-resolution, interpretable assessments from transcriptomic signals, phenotypes, variant databases, and the literature to prioritize potential diagnostic variants. In a rigorously curated benchmark of Undiagnosed Diseases Network (UDN) patients with paired genomic and transcriptomic data, RareCollab achieved 77% top-5 diagnostic accuracy and improved top-1 to top-5 accuracy by ~20% over widely used variant-prioritization approaches. RareCollab illustrates how modular artificial intelligence (AI) can operationalize multi-modal evidence for accurate, scalable rare disease diagnosis, offering a promising path toward reducing the diagnostic odyssey for affected families.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2602.04058v1"
  },
  {
    "id": 104,
    "title": "MARADONER: Motif Activity Response Analysis Done Right",
    "abstract": "Inferring the activities of transcription factors from high-throughput transcriptomic or open chromatin profiling, such as RNA-/CAGE-/ATAC-Seq, is a long-standing challenge in systems biology. Identification of highly active master regulators enables mechanistic interpretation of differential gene expression, chromatin state changes, or perturbation responses across conditions, cell types, and diseases. Here, we describe MARADONER, a statistical framework and its software implementation for motif activity response analysis (MARA), utilizing the sequence-level features obtained with pattern matching (motif scanning) of individual promoters and promoter- or gene-level activity or expression estimates. Compared to the classic MARA, MARADONER (MARA-done-right) employs an unbiased variance parameter estimation and a bias-adjusted likelihood estimation of fixed effects, thereby enhancing goodness-of-fit and the accuracy of activity estimation. Further, MARADONER is capable of accounting for heteroscedasticity of motif scores and activity estimates.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2602.03343v1"
  },
  {
    "id": 108,
    "title": "Accelerating De Novo Genome Assembly via Quantum-Assisted Graph Optimization with Bitstring Recovery",
    "abstract": "Genome sequencing is essential to decode genetic information, identify organisms, understand diseases and advance personalized medicine. A critical step in any genome sequencing technique is genome assembly. However, de novo genome assembly, which involves constructing an entire genome sequence from scratch without a reference genome, presents significant challenges due to its high computational complexity, affecting both time and accuracy. In this study, we propose a hybrid approach utilizing a quantum computing-based optimization algorithm integrated with classical pre-processing to expedite the genome assembly process. Specifically, we present a method to solve the Hamiltonian and Eulerian paths within the genome assembly graph using gate-based quantum computing through a Higher-Order Binary Optimization (HOBO) formulation with the Variational Quantum Eigensolver algorithm (VQE), in addition to a novel bitstring recovery mechanism to improve optimizer traversal of the solution space. A comparative analysis with classical optimization techniques was performed to assess the effectiveness of our quantum-based approach in genome assembly. The results indicate that, as quantum hardware continues to evolve and noise levels diminish, our formulation holds a significant potential to accelerate genome sequencing by offering faster and more accurate solutions to the complex challenges in genomic research.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2602.00156v1"
  },
  {
    "id": 117,
    "title": "PhageMind: Generalized Strain-level Phage Host Range Prediction via Meta-learning",
    "abstract": "Bacteriophages (phages) are key regulators of bacterial populations and hold great promise for applications such as phage therapy, biocontrol, and industrial fermentation. The success of these applications depends on accurately determining phage host range, which is often specific at the strain level rather than the species level. However, existing computational approaches face major limitations: many rely on genus-specific features that do not generalize across taxa, while others require large amounts of training data that are unavailable for most bacterial lineages. These challenges create a critical need for methods that can accurately predict strain-level phage-host interactions across diverse bacterial genera, particularly under data-limited conditions. We present PhageMind, a learning framework designed to address this challenge by enabling efficient transfer of knowledge across bacterial genera. PhageMind is trained to identify shared principles of phage-bacterium interactions from well-studied systems and to rapidly adapt these principles to new genera using only a small number of known interactions. To reflect the biological basis of infection, we represent phage-host relationships using a knowledge graph that explicitly incorporates phage tail fiber proteins and bacterial O-antigen biosynthesis gene clusters, and we use this representation to guide interaction prediction. Across four bacterial genera (Escherichia, Klebsiella, Vibrio, and Alteromonas), PhageMind achieves high prediction accuracy and shows strong adaptability to new lineages. In particular, in leave-one-genus-out evaluations, the model maintains robust performance when only limited reference data are available, demonstrating its potential as a scalable and practical tool for studying phage-host interactions across the global phageome.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.15886v1"
  },
  {
    "id": 122,
    "title": "engGNN: A Dual-Graph Neural Network for Omics-Based Disease Classification and Feature Selection",
    "abstract": "Omics data, such as transcriptomics, proteomics, and metabolomics, provide critical insights into disease mechanisms and clinical outcomes. However, their high dimensionality, small sample sizes, and intricate biological networks pose major challenges for reliable prediction and meaningful interpretation. Graph Neural Networks (GNNs) offer a promising way to integrate prior knowledge by encoding feature relationships as graphs. Yet, existing methods typically rely solely on either an externally curated feature graph or a data-driven generated one, which limits their ability to capture complementary information. To address this, we propose the external and generated Graph Neural Network (engGNN), a dual-graph framework that jointly leverages both external known biological networks and data-driven generated graphs. Specifically, engGNN constructs a biologically informed undirected feature graph from established network databases and complements it with a directed feature graph derived from tree-ensemble models. This dual-graph design produces more comprehensive embeddings, thereby improving predictive performance and interpretability. Through extensive simulations and real-world applications to gene expression data, engGNN consistently outperforms state-of-the-art baselines. Beyond classification, engGNN provides interpretable feature importance scores that facilitate biologically meaningful discoveries, such as pathway enrichment analysis. Taken together, these results highlight engGNN as a robust, flexible, and interpretable framework for disease classification and biomarker discovery in high-dimensional omics contexts.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.14536v1"
  },
  {
    "id": 125,
    "title": "GP-DHT: A Dual-Head Transformer with Contras-tive Learning for Predicting Gene Regulatory Rela-tionships across Species from Single-Cell Data",
    "abstract": "Gene regulatory networks (GRNs) are essential for understanding cell fate decisions and disease mechanisms, yet cross-species GRN inference from single-cell RNA-seq data remains challenging due to noise, sparsity, and cross-species distribution shifts. We propose GP-DHT (GenePair DualHeadTransformer), a cross-species single-cell GRN inference framework that models genes and cells in a heterogeneous graph with multi-level expression relations and learns structured regulatory representations via multi-relational graph attention. A dual-head Transformer further captures local gene pair regulatory dependencies and global cross-cell interaction patterns. To improve robustness under sparse and cross-species settings, GP-DHT introduces gene pair level supervised contrastive learning. Experiments on seven BEELINE benchmark datasets show consistent gains over representative baselines, improving AUROC and AUPRC by approximately 5 to 7 percent on most datasets. GP-DHT also recovers known regulatory modules and helps distinguish conserved from species-specific regulations.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.10995v1"
  },
  {
    "id": 370,
    "title": "Learning virulence-transmission relationships using causal inference",
    "abstract": "The relationship between traits that influence pathogen virulence and transmission is part of the central canon of the evolution and ecology of infectious disease. However, identifying directional and mechanistic relationships among traits remains a key challenge in various subfields of biology, as models often assume static, fixed links between characteristics. Here, we introduce learning evolutionary trait relationships (LETR), a data-driven framework that applies Granger-causality principles to determine which traits drive others and how these relationships change over time. LETR integrates causal discovery with generative mapping and transfer-operator analysis to link short-term predictability with long-term trait distributions. Using a synthetic myxomatosis virus-host data set, we show that LETR reliably recovers known directional influences, such as virulence driving transmission. Applying the framework to global pandemic (SARS-CoV-2) data, we find that past virulence improves future transmission prediction, while the reverse effect is weak. Invariant-density estimates reveal a long-term trend toward low virulence and transmission, with bimodality in virulence suggesting ecological influences or host heterogeneity. In summary, this study provides a blueprint for learning the relationship between how harmful a pathogen is and how well it spreads, which is highly idiosyncratic and context-dependent. This finding undermines simplistic models and encourages the development of new theory for the constraints underlying pathogen evolution. Further, by uniting causal inference with dynamical modeling, the LETR framework offers a general approach for uncovering mechanistic trait linkages in complex biological systems of various kinds.",
    "domain": "q-bio",
    "subdomain": "q-bio.QM",
    "arxiv_id": "2602.05196v1"
  },
  {
    "id": 460,
    "title": "Microbiome association diversity reflects proximity to the edge of instability",
    "abstract": "Recent advances in metagenomics have revealed macroecological patterns or \"laws\" describing robust statistical regularities across microbial communities. Stochastic logistic models (SLMs), which treat species as independent -- akin to ideal gases in physics -- and incorporate environmental noise, reproduce many single-species patterns but cannot account for the pairwise covariation observed in microbiome data. Here we introduce an interacting stochastic logistic model (ISLM) that minimally extends the SLM by sampling an ensemble of random interaction networks chosen to preserve these single-species laws. Using dynamical mean-field theory, we map the model's phase diagram -- stable, chaotic, and unbounded-growth regimes -- where the transition from stable fixed-point to chaos is controlled by network sparsity and interaction heterogeneity via a May-like instability line. Going beyond mean-field theory to account for finite communities, we derive an estimator of an effective stability parameter that quantifies distance to the edge of instability and can be inferred from the width of the distribution of pairwise covariances in empirical species-abundance data. Applying this framework to synthetic data, environmental microbiomes, and human gut cohorts indicates that these communities tend to operate near the edge of instability. Moreover, gut communities from healthy individuals cluster closer to this edge and exhibit broader, more heterogeneous associations, whereas dysbiosis-associated states shift toward more stable regimes -- enabling discrimination across conditions such as Crohn's disease, inflammatory bowel syndrome, and colorectal cancer. Together, our results connect macroecological laws, interaction-network ensembles, and May's stability theory, suggesting that complex communities may benefit from operating near a dynamical phase transition.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.22918v1"
  },
  {
    "id": 462,
    "title": "Multi-strain SIS dynamics with coinfection under host population structure",
    "abstract": "Coinfection phenomena are common in nature, yet there is a lack of analytical approaches for coinfection systems with a high number of circulating and interacting strains. In this paper, we investigated a coinfection SIS framework applied to N strains, co-circulating in a structured host population. Adopting a general formulation for fixed host classes, defined by arbitrary epidemiological traits such as class-specific transmission rates, susceptibilities, clearance rates, etc., our model can be easily applied in different frameworks: for example, when different host species share the same pathogen, in classes of vaccinated or non-vaccinated hosts, or even in classes of hosts defined by the number of contacts. Using the strain similarity assumption, we identify the fast and slow variables of the epidemiological dynamics on the host population, linking neutral and non-neutral strain dynamics, and deriving a global replicator equation. This global replicator equation allows to explicitly predict coexistence dynamics from mutual invasibility coefficients among strains. The derived global pairwise invasion fitness matrix contains explicit traces of the underlying host population structure, and of its entanglement with the strain interaction and trait landscape. Our work thus enables a more comprehensive study and efficient simulation of multi-strain dynamics in endemic ecosystems, paving the way to deeper understanding of global persistence and selection forces, jointly shaped by pathogen and host diversity.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2602.00193v1"
  },
  {
    "id": 464,
    "title": "Impact of behavioral heterogeneity on epidemic outcome and its mapping into effective network topologies",
    "abstract": "Human behavior plays a critical role in shaping epidemic trajectories. During health crises, people respond in diverse ways in terms of self-protection and adherence to recommended measures, largely reflecting differences in how individuals assess risk. This behavioral variability induces effective heterogeneity into key epidemic parameters, such as infectivity and susceptibility. We introduce a minimal extension of the susceptible-infected-removed~(SIR) model, denoted HeSIR, that captures these effects through a simple bimodal scheme, where individuals may have higher or lower transmission--related traits. We derive a closed-form expression for the epidemic threshold in terms of the model parameters, and the network's degree distribution and homophily, defined as the tendency of like--risk individuals to preferentially interact. We identify a resurgence regime just beyond the classical threshold, where the number of infected individuals may initially decline before surging into large-scale transmission. Through simulations on homogeneous and heterogeneous network topologies we corroborate the analytical results and highlight how variations in susceptibility and infectivity influence the epidemic dynamics. We further show that, under suitable assumptions, the HeSIR model maps onto a standard SIR process on an appropriately modified contact network, providing a unified interpretation in terms of structural connectivity. Our findings quantify the effect of heterogeneous behavioral responses, especially in the presence of homophily, and caution against underestimating epidemic potential in fragmented populations, which may undermine timely containment efforts. The results also extend to heterogeneity arising from biological or other non-behavioral sources.",
    "domain": "q-bio",
    "subdomain": "q-bio.PE",
    "arxiv_id": "2601.21743v1"
  },
  {
    "id": 513,
    "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
    "abstract": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.",
    "domain": "q-bio",
    "subdomain": "q-bio.BM",
    "arxiv_id": "2602.00663v1"
  },
  {
    "id": 521,
    "title": "Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules",
    "abstract": "Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.",
    "domain": "q-bio",
    "subdomain": "q-bio.BM",
    "arxiv_id": "2601.18716v1"
  },
  {
    "id": 522,
    "title": "The Quantum Cliff: A Critical Proton Tunneling Threshold Determines Clinical Severity in RPE65-Mediated Retinal Disease",
    "abstract": "Predicting clinical severity from genotype remains a fundamental challenge in molecular medicine, particularly for enzymes whose function depends on sub-atomic-scale geometry. Mutations in the \\textit{RPE65} isomerohydrolase cause Leber Congenital Amaurosis (LCA) and related retinal diseases; however, the kinetic mechanisms connecting sub-atomic-scale perturbations to blindness remain unclear. In this study, we demonstrate that mutations in the human visual isomerase RPE65 are governed by a quantum-mechanical threshold effect arising from proton tunneling in the active site. We established a hybrid quantum-classical structure-to-phenotype pipeline combining AlphaFold structure prediction with \\textit{ab initio} quantum simulation using the Variational Quantum Eigensolver (VQE) to analyze minimal proton-coupled electron transfer in the visual cycle. Our analysis reveals that many pathogenic mutations do not merely occlude the active site, but rather strongly reduce the quantum probability of proton tunneling. We observed a sharp non-linear effect, termed the \"Quantum Cliff,\" where minute structural changes (below 0.1 \u00c5) reduce the reaction rate by multiple orders of magnitude. Based on these findings, we introduce a dimensionless Relative Quantum Activity Score (RQAS) that isolates the geometry-controlled exponential sensitivity of the reaction rate and successfully distinguishes between mild and severe patient phenotypes. These results suggest that RPE65 operates near a quantum-critical point, where sub-Angstrom structural perturbations induce a catastrophic loss of function. Furthermore, our findings establish quantum tunneling as a predictive mechanistic link between atomic structure and clinical phenotype, proposing a general framework for quantum-structural disease modeling.",
    "domain": "q-bio",
    "subdomain": "q-bio.BM",
    "arxiv_id": "2601.18435v1"
  },
  {
    "id": 524,
    "title": "A novel scalable high performance diffusion solver for multiscale cell simulations",
    "abstract": "Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.",
    "domain": "q-bio",
    "subdomain": "q-bio.CB",
    "arxiv_id": "2602.05017v1"
  },
  {
    "id": 525,
    "title": "Effects of multi-phase control mechanism on fibroblast dynamics: A segmented mathematical modeling approach",
    "abstract": "Cell size is a fundamental determinant of cellular physiology, influencing processes such as growth, division, and function. In this study, we develop a segmented mathematical framework to investigate how different control mechanisms operating across multiple phases of the cell cycle affect fibroblast population dynamics. Building on our previous work modeling sizer, timer, and adder strategies, we extend the analysis by introducing phase-specific control schemes in the S and G2 phases, incorporating nonlinear growth dynamics and cell death. Using agent-based stochastic simulations, we examine how these mechanisms shape steady-state size distributions and respond to parameter variations. Our results reveal that the steady-state cell size distribution is primarily governed by division kernels and phase-specific control strategies, and appears remarkably robust to cell death modalities. We identify a fundamental trade-off between extrinsic and intrinsic growth feedbacks: while population-density-dependent regulation tightly limits total cell numbers, cell-size-dependent regulation acts as a proportional homeostatic mechanism, suppressing relative size variability. Furthermore, we demonstrate that population recovery is accelerated by the retention of proliferation-competent large cells. This study provides biologically relevant insights into the complex interplay between growth, division, and homeostasis, with implications for understanding tissue repair and disease progression.",
    "domain": "q-bio",
    "subdomain": "q-bio.CB",
    "arxiv_id": "2601.22613v1"
  },
  {
    "id": 542,
    "title": "Compartmental-reaction diffusion framework for microscale dynamics of extracellular serotonin in brain tissue",
    "abstract": "Serotonin (5-hydroxytryptamine) is a major neurotransmitter whose release from densely distributed serotonergic varicosities shapes plasticity and network integration throughout the brain, yet its extracellular dynamics remain poorly understood due to the sub-micrometer and millisecond scales involved. We develop a mathematical framework that captures the coupled reaction-diffusion processes governing serotonin signaling in realistic tissue microenvironments. Formulating a two-dimensional compartmental-reaction diffusion system, we use strong localized perturbation theory to derive an asymptotically equivalent set of nonlinear integro-ODEs that preserve diffusive coupling while enabling efficient computation. We analyze period-averaged steady states, establish bounds using Jensen's inequality, obtain closed-form spike maxima and minima, and implement a fast marching-scheme solver based on sum-of-exponentials kernels. These mathematical results provide quantitative insight into how firing frequency, varicosity geometry, and uptake kinetics shape extracellular serotonin. The model reveals that varicosities form diffusively coupled microdomains capable of generating spatial \"serotonin reservoirs,\" clarifies aspects of local versus volume transmission, and yields predictions relevant to interpreting high-resolution serotonin imaging and the actions of selective serotonin-reuptake inhibitors.",
    "domain": "q-bio",
    "subdomain": "q-bio.CB",
    "arxiv_id": "2512.10983v1"
  },
  {
    "id": 543,
    "title": "Cell-cell communication inference and analysis: biological mechanisms, computational approaches, and future opportunities",
    "abstract": "In multicellular organisms, cells coordinate their activities through cell-cell communication (CCC), which are crucial for development, tissue homeostasis, and disease progression. Recent advances in single-cell and spatial omics technologies provide unprecedented opportunities to systematically infer and analyze CCC from these omics data, either by integrating prior knowledge of ligand-receptor interactions (LRIs) or through de novo approaches. A variety of computational methods have been developed, focusing on methodological innovations, accurate modeling of complex signaling mechanisms, and investigation of broader biological questions. These advances have greatly enhanced our ability to analyze CCC and generate biological hypotheses. Here, we introduce the biological mechanisms and modeling strategies of CCC, and provide a focused overview of more than 140 computational methods for inferring CCC from single-cell and spatial transcriptomic data, emphasizing the diversity in methodological frameworks and biological questions. Finally, we discuss the current challenges and future opportunities in this rapidly evolving field.",
    "domain": "q-bio",
    "subdomain": "q-bio.CB",
    "arxiv_id": "2512.03497v1"
  },
  {
    "id": 719,
    "title": "Mathematical Modeling of Lesion Pattern Formation in Dendritic Keratitis",
    "abstract": "Dendritic keratitis is a form of eye infection caused by herpes simplex virus (HSV). The virus spreads via direct cell-to-cell infection among corneal epithelial cells. This leads to the formation of dendritic lesions characterized by terminal bulbs at their tips. Under immunosuppression, the condition may progress to geographic keratitis, which is a map-shaped lesion with dendritic tails. The mechanism of this pattern formation remains to be elucidated. In this study, we propose a mathematical model to elucidate the mechanisms of lesion pattern formation in dendritic keratitis. Our model shows that increased production of infection-suppressive cytokines induces dendritic patterns with terminal bulbs, whereas reduced cytokine levels lead to geographic patterns. Furthermore, altering the spatial distribution of cytokine production can reproduce dendritic tails. By including external cytokine secretion, we could reproduce tapered lesions observed in non-HSV keratitis. By clarifying the mechanisms behind terminal bulb formation and reproducing atypical lesion morphologies, our findings enhance the understanding of herpetic keratitis and highlight the utility of mathematical modeling in ophthalmology.",
    "domain": "q-bio",
    "subdomain": "q-bio.TO",
    "arxiv_id": "2602.02916v2"
  },
  {
    "id": 724,
    "title": "From the Hallmarks of Cancer to the Survival System: Integration and Paradigmatic Reconstruction of Classical Oncology Theories by the Tumor Existential Crisis-Driven Survival (ECDS) Theory",
    "abstract": "Malignant tumors are defined by extraordinarily intricate pathogenic mechanisms, yet classical oncological theories remain fragmented in an archipelagic state-lacking a unifying framework to integrate the processes of tumor initiation, progression, metastasis, and therapeutic resistance. This theoretical disjuncture constrains the efficacy of the confront-and-eradicate paradigm, culminating in limited therapeutic breakthroughs, recurrent drug resistance, and substantial host toxicity. We herein propose the ECDS (Tumor Existential Crisis-Driven Survival) theory, anchored in a central mechanistic axis: impairment of Existential Stability elicits compensatory hyperactivation of Survival Capacity. The ECDS framework establishes three foundational constructs (Existential Stability, Survival Capacity, and Existence Threshold) alongside three core principles, integrating pivotal theories of tumorigenesis. It furnishes a systematic account of the dynamic coupling between augmented survival capacity and declining existential stability throughout tumor evolution; reinterprets the fundamental nature and temporal hierarchy of the fourteen cancer hallmarks; clarifies the redundancy of survival pathways in tumor heterogeneity; and elucidates the adaptive underpinnings of therapeutic resistance. As a holistic integrative model, ECDS offers transformative conceptual insights for oncology by reframing tumors not merely as abnormally-proliferating cell aggregates but as biological subsystems driven into survival mode by eroding existential stability. This challenges the prevailing dogma of tumors as intrinsically aggressive entities, revealing their core traits of intrinsic vulnerability and passive adaptation, while laying a meta-theoretical foundation for reorienting cancer study and management toward threshold-regulated dynamic adaptation.",
    "domain": "q-bio",
    "subdomain": "q-bio.TO",
    "arxiv_id": "2601.09767v1"
  },
  {
    "id": 725,
    "title": "Simulations Predict Improved Valve Performance Without Direct Leaflet Intervention After Neonatal Truncus Arteriosus Repair",
    "abstract": "Truncus arteriosus (TA) is a rare and severe congenital heart disease. Quadricuspid valve morphology occurs in 25% of all TA patients and is linked to regurgitation and increased risk of re-operation. It remains unclear how hemodynamic changes after TA repair alter valve performance. This study simulated pre- and postoperative conditions in a neonatal TA patient to investigate valve performance without direct intervention. We hypothesize that valve performance before and after truncal repair can be predicted in-silico, matching in-vivo imaging and identifying mechanisms how hemodynamic changes after repair will reduce valve regurgitation without direct intervention. Pre- and postoperative CT images of a neonatal patient with quadricuspid valve were segmented. Free edge length and geometric height from the patient's echocardiogram were used to model the valve. For the preoperative condition, ventricular pressures were set equal modeling an unrestricted ventricular septal defect. Systemic and pulmonary resistances were tuned based on the patient's Qp:Qs ratio. For the postoperative condition, boundary conditions were modified to mimic patient-specific hemodynamics after TA repair. The preoperative simulation confirmed mild valve regurgitation seen in-vivo. Interaction between asymmetric flow and surrounding vessel resulted in asymmetric opening and closing. Poor central coaptation led to a central regurgitant jet toward the septum. Altered postoperative hemodynamics improved coaptation and eliminated regurgitation, as seen in-vivo. This modeling approach reproduced in-vivo pre- and postoperative valve performance and identified mechanisms improving coaptation after TA repair. TA repair led to elimination of regurgitation due to enhanced central coaptation. Thus, altered postoperative hemodynamic conditions after TA repair may improve valve performance without direct leaflet intervention.",
    "domain": "q-bio",
    "subdomain": "q-bio.TO",
    "arxiv_id": "2601.08932v1"
  },
  {
    "id": 1,
    "title": "The Hubble Tension in Light of the Symmetry of Scale Invariance",
    "abstract": "When the expansion rate of the Universe at recombination is used to infer the present expansion rate $H_0$, the value derived in the $\u039b$CDM model, $H_0=67.4$ km/s/mpc, is about in 6$\\, \u03c3$ tension with the value measured locally, $H_0=74$ km/s/mpc. In this work, we consider instead the expansion history in the context of the symmetry of scale-invariant vacuum (SIV model). We first perform two major cosmological tests: the Hubble diagram for type-Ia supernovae and the fundamental relation between $H_0$, the age of the Universe, and the total density of matter, $\u03a9_m$. This allows us to constrain $\u03a9_m$ in SIV, with both tests giving the best agreement for $\u03a9_m \\simeq 0.20$. We then study the physical connections of the dynamical and thermal states of the Universe at recombination with the present Hubble constant, $H_0$, and the present temperature, $T$, in the $\u039b$CDM and SIV contexts. We find that, in SIV, the properties at recombination may be conveyed to the present ones ($T=2.726$ and $H_0$ at $z=0$) without any tension, indicating $H_0=74$ km/s/mpc in spite of the anchoring on the CMB. This is due to the slightly different expansion and temperature histories of the two cosmological models. Importantly, this happens to occur for $\u03a9_m \\simeq 0.20$, as constrained in SIV with supernovae and cosmic age. This suggests that the Hubble tension currently found between $H_0$ values in the early and late Universe may simply be the result of $\u039bCDM$ ignoring the small but still measurable effects of scale invariance.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2602.04532v1"
  },
  {
    "id": 2,
    "title": "Discrete dynamical systems with scaling and inversion symmetries",
    "abstract": "In this work, we investigate scale invariance in the temporal evolution and chaotic regime of discrete dynamical systems. By exploiting the close interrelation between scaling and inversion transformations, we formulate scale symmetry in terms of inversion symmetry. As applications of our approach, we determine fractal dimensions and compute Lyapunov exponents for paradigmatic dynamical systems using scaling and inversion symmetries. By comparing our method with standard approaches, we obtain identical numerical values for the Lyapunov exponents using only a small number of iterations. Furthermore, our geometric-based framework naturally provides access to the fractal dimension. The agreement with standard results demonstrates that the proposed method is efficient and can be effectively employed in the study of dynamical systems.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2602.02622v1"
  },
  {
    "id": 3,
    "title": "Dynamical Oscillations in Dark Energy: Joint Constraints on the $w_{sin}$CDM Model from DESI, OHD, and Supernova Samples",
    "abstract": "In this study, we investigate the oscillatory dark energy model $w_{\\sin}\\mathrm{CDM}$ based on the DESI BAO data together with OHD, Pantheon Plus, and SH0ES measurements. We examine how the DESI data influence the dark energy equation-of-state plane $(w_0, w_a)$ within cosmological models that are free from Hubble tension and employ a Monte Carlo Markov Chain (MCMC) approach. Our findings indicate that although the parameter space still favors $w_a < 0$ and $w_0 > -1$ , the cosmological constant remains consistent with the DESI+OHD+PP combination at the $2\u03c3$ level. We also observe that the best-fit Hubble constant $H_0$ is higher for the DESI+OHD+PP+SH0ES data combination, leading to a residual Hubble tension of less than $1\u03c3$ to remain consistent with the SH0ES measurement. These results suggest that attempts to address the Hubble tension tend to reduce indication of DESI for the oscillatory dark energy model. Therefore, claims that the cosmological constant should be approached with greater caution, considering both the latest observational datasets and the existing cosmological tensions. We also obtained the present deceleration parameter and the effective equation-of-state value as $q_0 = -0.36$ and $w_{\\mathrm{eff}} = -0.57$, respectively, for the DESI+OHD+PP+SH0ES dataset combination. Further analysis indicated a strong departure of $w_0$ from $w=-1$ at the $4\u03c3$ level for the DR2+OHD+DES-5yr data combination. The inferred $\u03a9_{m}$ tended to shift toward higher values when supernova samples were included, indicating a systematic preference for larger $\u03a9_{m}$ in combinations involving supernova data.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2602.04887v1"
  },
  {
    "id": 4,
    "title": "Statistical physics on Euclidean Snyder space: connections with the GUP and cosmological implications",
    "abstract": "We develop a systematic formulation of statistical mechanics on Euclidean Snyder space, where noncommutativity is geometrically encoded in the curvature of momentum space. Adopting a realization independent approach based on momentum-space invariants, we derive modified partition functions and thermodynamic quantities for systems obeying Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac statistics in both non-relativistic and ultrarelativistic regimes. We show that momentum-space curvature induces temperature-dependent corrections that suppress the energy, entropy and energy density with respect to their standard counterparts. We apply these results to early-Universe cosmology, deriving the corresponding corrections to the Friedmann equations driven by the modified energy density of radiation. Using Big Bang Nucleosynthesis as a precision probe, we derive bounds on the Snyder deformation parameter and, via a phenomenological mapping, on the Generalized Uncerainty Principle (GUP) parameter, providing one of the most stringent cosmological and astrophysical constraints currently available. Our analysis demonstrates that high-energy cosmological processes provide a predictive arena for testing momentum-space curvature and noncommutative geometry effects.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2602.02506v1"
  },
  {
    "id": 5,
    "title": "Scaling-Based Quantization of Spacetime Microstructure",
    "abstract": "Planck-scale physics challenges the classical smooth-spacetime picture by introducing quantum fluctuations that imply a nontrivial spacetime microstructure. We present a framework that encodes these fluctuations by promoting local scale factors, rather than the metric tensor, to fundamental dynamical variables while preserving general covariance. The construction employs a two-tiered hierarchy of scale manifolds, comprising a first-order manifold of scale coordinates and a second-order manifold of fluctuation amplitude coordinates. On the first-order manifold, we formulate differential geometry, field equations, and a canonical quantization procedure. The theory yields a geometric renormalization-group flow for scale variables and implies spacetime discreteness at the microscopic level. By constructing a quadratic action and performing spectral decomposition with a stabilizing potential, we obtain discrete modal degrees of freedom quantized as harmonic oscillators. The framework proposes a microscopic description for zero-point energy of spacetime and explores implications for vacuum energy and ultraviolet regularization, suggesting a potential dynamical mechanism that could ameliorate the cosmological constant problem. Main results include a generalized uncertainty relation with scale-dependent coefficients, locally scaled Klein-Gordon and Dirac equations, geodesic equations for scale spacetime, and a microscopic area operator whose state counting is consistent with the Bekenstein-Hawking entropy. This work develops a scale-based quantization procedure, providing a foundation for further mathematical analysis and phenomenological tests of spacetime quantization.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2601.15649v1"
  },
  {
    "id": 7,
    "title": "Metastable Transitions and $\u0393$-Convergent Eyring-Kramers Asymptotics in Landau-QCD Gradient Systems",
    "abstract": "We develop a rigorous analytical framework for metastable stochastic transitions in Landau-type gradient systems inspired by QCD phenomenology. The functional $F(\u03c3;u)=\\int_\u03a9[\\frac\u03ba{2}|\\nabla\u03c3|^2+V(\u03c3;u)]\\,dx$, depending smoothly on a control parameter $u\\in\\mathcal U$, is analyzed through the Euler-Lagrange map $\\mathcal{E}(\u03c3;u)=-\u03ba\u0394\u03c3+V'(\u03c3;u)$ and its Hessian $\\mathcal{L}_{\u03c3,u}=-\u03ba\u0394+V''(\u03c3;u)$. By combining variational methods, $\u0393$- and Mosco convergence, and spectral perturbation theory, we establish the persistence and stability of local minima and index-one saddles under parameter deformations and variational discretizations. The associated mountain-pass solutions form Cerf-continuous branches away from the discriminant set $\\mathcal D=\\{u:\\det\\mathcal L_{\u03c3,u}=0\\}$, whose crossings produce only fold or cusp catastrophes in generic one- and two-parameter slices. The $\u0393$-limit is taken with respect to the $L^2(\u03a9)$ topology, ensuring compactness, convergence of gradient flows, and spectral continuity of $\\mathcal L_{\u03c3,u}$. As a consequence, the Eyring-Kramers formula for the mean transition time between metastable wells retains quantitative validity under both parameter deformations and discretization refinement, with convergent free-energy barriers, unstable eigenvalues, and zeta-regularized determinant ratios. This construction unifies the classical intuition of Eyring, Kramers, and Langer with modern variational and spectral analysis, providing a mathematically consistent and physically transparent foundation for metastable decay and phase conversion in Landau-QCD-type systems.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2601.15343v1"
  },
  {
    "id": 8,
    "title": "Z(3) Metastable Bubbles and Chiral Dynamics Across a Dark-QCD Deconfinement Transition",
    "abstract": "We present a self-contained theoretical analysis of a dark-QCD chiral transition in which the Polyakov-loop sector retains an explicit $Z(3)$ structure and couples consistently to the chiral order parameter.Working within a coupled chiral--Polyakov effective theory, we map the homogeneous vacuum landscape and identify a metastability window bounded by spinodal loss of stability.We then construct $Z(3)$ domain-wall solutions including chiral backreaction, extracting temperature-dependent wall profiles and surface tension.Finally, we connect homogeneous metastability and wall microphysics to thermal bubble nucleation by evaluating the critical radius $R_c(T)$ and the nucleation exponent $S_3(T)/T$ in the thin-wall regime, providing a compact set of reproducible diagnostics for the decay of the metastable phase.Our results establish a coherent pipeline from vacuum structure to nonperturbative interfaces and nucleation barriers, suitable for systematic extensions to full multi-field bounce calculations and dark-sector cosmological applications.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2601.15342v1"
  },
  {
    "id": 9,
    "title": "Minimal Proper-time in Quantum Field Theory",
    "abstract": "We propose a generalization of quantum field theory within Schr\u00f6dinger's functional representation, inspired by Nambu's proper-time formulation of quantum mechanics. The key motivation for this generalization is to incorporate a fundamental, Lorentz-invariant minimum scale, which in this formulation is played by a minimal proper time $\u03c4_{\\min}$. The introduction of $\u03c4_{\\min}$ leads to several significant effects at very high energies: it modifies the Heisenberg uncertainty principle, induces a controlled violation of unitarity, and suppresses high-energy modes. This minimal scale renders the theory asymptotically safe through a mechanism akin to dimensional reduction, while reproducing all the standard results at low energies, where quantum field theory emerges. Remarkably, the same framework can accommodate a deterministic regime at energies approaching the Planck scale. These features suggest that a minimal proper-time formulation renders quantum field theory an effective but finite theory, superseded at trans-Planckian energies.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2602.00045v1"
  },
  {
    "id": 10,
    "title": "Compressing Complexity: A Critical Synthesis of Structural, Analytical, and Data-Driven Dimensionality Reduction in Dynamical Networks",
    "abstract": "The contemporary scientific landscape is characterized by a \"curse of dimensionality,\" where our capacity to collect high-dimensional network data frequently outstrips our ability to computationally simulate or intuitively comprehend the underlying dynamics. This review provides a comprehensive synthesis of the methodologies developed to resolve this paradox by extracting low-dimensional \"macroscopic theories\" from complex systems. We classify these approaches into three distinct methodological lineages: Structural Coarse-Graining, which utilizes spectral and topological renormalization to physically contract the network graph; Analytical-Based Reduction, which employs rigorous ansatzes (such as Watanabe-Strogatz and Ott-Antonsen) and moment closures to derive reduced differential equations ; and Data-Driven Reduction, which leverages manifold learning and operator-theoretic frameworks (e.g., Koopman analysis) to infer latent dynamics from observational trajectories. We posit that the selection of a reduction strategy is governed by a fundamental \"No Free Lunch\" theorem, establishing a Pareto frontier between computational tractability and physical fidelity. Furthermore, we identify a growing epistemological schism between equation-based derivations that preserve causal mechanisms and black-box inference that prioritizes prediction. We conclude by discussing emerging frontiers, specifically the necessity of Higher-Order Laplacian Renormalization for simplicial complexes and the development of hybrid \"Scientific Machine Learning\" architectures-such as Neural ODEs-that fuse analytical priors with deep learning to solve the closure problem.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2602.00039v1"
  },
  {
    "id": 13,
    "title": "Constraint-Induced Effective Mass in Massless Field Propagation",
    "abstract": "Constrained propagation of massless fields is ubiquitous in physical systems, arising from boundaries, material structure, or other restrictions on admissible modes. This paper shows that such constraints generically induce mass-like terms in the effective dispersion relation, without modifying the underlying field equations or introducing new degrees of freedom. Working at an abstract level, constraints are represented as linear operators acting on the field's mode space. Restriction of the admissible mode manifold produces a spectral gap whose magnitude is set by the smallest non-zero eigenvalue of an associated positive semidefinite operator. This gap may be identified with an effective mass parameter, yielding a Proca-like dispersion relation in the long-wavelength limit. The resulting Mass Induction Principle identifies rank reduction of the accessible mode space as the structural mechanism responsible for effective mass generation in constrained massless fields. Familiar systems such as plasmas, superconductors, and periodic media realise this structure as special cases, without introducing new dynamics. The analysis is deliberately dispersion-level and non-phenomenological: it does not assert a field-theoretic mass term, does not address vacuum propagation, and does not make claims about bounds on intrinsic particle masses.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2601.09642v1"
  },
  {
    "id": 14,
    "title": "A Space-Time Fluid (Unabridged)",
    "abstract": "Purpose: This essay is a retelling of general relativity in a language in which space-time geometry is expressed as a fluid. This trivial and useful reformulation gives 1) a non-perturbative covariant description of cosmological inhomogeneities and 2) a simple formula describing how cosmic inhomogeneities are generated on super-horizon scales.\n  Methods: Equating the Ricci curvature with the associated matter stress-energy gives a description of space-time geometry in terms of fluid properties. These locally measurable (covariant) non-perturbative quantities are in some ways superior to commonly used \"gauge invariant\" quantities. The dynamics of a quantity (kurvature) which describes cosmological inhomogeneities is described in detail. A detailed comparison is made of space-time fluid dynamics with that of a classical (Newtonian physics) fluid.\n  Results: The fluid lexicon permits an unambiguous definition of the velocity of space-time. The evolution of the space-time fluid is in many ways identical with that of the classical fluid when expressed in Lagrangian coordinates. Kurvature is a measure of the specific binding energy of the fluid and is a most useful covariant measure of cosmological inhomogeneities. For plausible matter models kurvature will increase, even on super-horizon scales, due to non-linear hydrodynamic effects rather than gravity. This phenomena is also exhibited by classical fluids.\n  Conclusion: The space-time fluid representation of geometrodynamics gives a simple and useful description of the evolution of cosmological inhomogeneities.",
    "domain": "physics",
    "subdomain": "physics.gen-ph",
    "arxiv_id": "2601.16996v1"
  },
  {
    "id": 16,
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "abstract": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
    "domain": "cs",
    "subdomain": "cs.AI",
    "arxiv_id": "2602.06043v1"
  },
  {
    "id": 19,
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "abstract": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
    "domain": "cs",
    "subdomain": "cs.AI",
    "arxiv_id": "2602.06025v1"
  },
  {
    "id": 20,
    "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
    "abstract": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.",
    "domain": "cs",
    "subdomain": "cs.AI",
    "arxiv_id": "2602.06023v1"
  },
  {
    "id": 22,
    "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
    "abstract": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \\citep{halder2025stable} is stable for any $K \\ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.",
    "domain": "cs",
    "subdomain": "cs.AI",
    "arxiv_id": "2602.06014v1"
  },
  {
    "id": 29,
    "title": "Clifford Kolmogorov-Arnold Networks",
    "abstract": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.",
    "domain": "cs",
    "subdomain": "cs.AI",
    "arxiv_id": "2602.05977v1"
  },
  {
    "id": 30,
    "title": "Inverse Depth Scaling From Most Layers Being Similar",
    "abstract": "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.",
    "domain": "cs",
    "subdomain": "cs.AI",
    "arxiv_id": "2602.05970v1"
  },
  {
    "id": 37,
    "title": "BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction",
    "abstract": "Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. To address these challenges, we introduce BrainVista, a multimodal autoregressive framework designed to model the causal evolution of brain states. BrainVista incorporates Network-wise Tokenizers to disentangle system-specific dynamics and a Spatial Mixer Head that captures inter-network information flow without compromising functional boundaries. Furthermore, we propose a novel Stimulus-to-Brain (S2B) masking mechanism to synchronize high-frequency sensory stimuli with hemodynamically filtered signals, enabling strict, history-only causal conditioning. We validate our framework on Algonauts 2025, CineBrain, and HAD, achieving state-of-the-art fMRI encoding performance. In long-horizon rollout settings, our model yields substantial improvements over baselines, increasing pattern correlation by 36.0\\% and 33.3\\% on relative to the strongest baseline Algonauts 2025 and CineBrain, respectively.",
    "domain": "biology",
    "subdomain": "q-bio.NC",
    "arxiv_id": "2602.04512v1"
  },
  {
    "id": 42,
    "title": "A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model",
    "abstract": "Adaptive cognition requires structured internal models representing objects and their relations. Predictive neural networks are often proposed to form such \"world models\", yet their underlying mechanisms remain unclear. One hypothesis is that action-conditioned sequential prediction suffices for learning such world models. In this work, we investigate this possibility in a minimal in-silico setting. Sequentially sampling tokens from 2D continuous token scenes, a recurrent neural network is trained to predict the upcoming token from current input and a saccade-like displacement. On novel scenes, prediction accuracy improves across the sequence, indicating in-context learning. Decoding analyses reveal path integration and dynamic binding of token identity to position. Interventional analyses show that new bindings can be learned late in sequence and that out-of-distribution bindings can be learned. Together, these results demonstrate how structured representations that rely on flexible binding emerge to support prediction, offering a mechanistic account of sequential world modeling relevant to cognitive science.",
    "domain": "biology",
    "subdomain": "q-bio.NC",
    "arxiv_id": "2602.03490v1"
  },
  {
    "id": 54,
    "title": "Matrices of nonnegative integer rank two",
    "abstract": "The nonnegative integer rank of a matrix is a variant of the classical nonnegative rank, introduced in the 1980s, where factorizations are required to have integer entries. While computing nonnegative integer rank is generally very hard, we focus on a fundamental special case: determining when a rank 2 nonnegative integer matrix has nonnegative integer rank equal to 2 (the \"rank2 problem\"). Although this problem is trivial in the continuous case, in this context it is surprisingly rich.\n  We provide a geometric reformulation in terms of affine semigroups and rational cones in the plane, which yields new structural insights. We show that any rank 2 integer matrix can be reduced to a $3\\times 3$ matrix which has nonnegative integer rank $2$ if and only if the original one also has nonnegative integer rank $2$, with the reduction computable in polynomial time. This reduction reveals that the difficulty of the rank2 problem is already captured by small matrices. Building on this geometric framework, we also develop an algorithm that solves the rank2 problem by strategically searching for integer generators within bounded regions of the associated cone. Although the theoretical worst-case complexity remains high, numerical experiments demonstrate that the algorithm performs efficiently in practice.",
    "domain": "math",
    "subdomain": "math.CO",
    "arxiv_id": "2602.05957v1"
  },
  {
    "id": 69,
    "title": "Winning in the Limit: Average-Case Committee Selection with Many Candidates",
    "abstract": "We study the committee selection problem in the canonical impartial culture model with a large number of voters and an even larger candidate set. Here, each voter independently reports a uniformly random preference order over the candidates. For a fixed committee size $k$, we ask when a committee can collectively beat every candidate outside the committee by a prescribed majority level $\u03b1$. We focus on two natural notions of collective dominance, $\u03b1$-winning and $\u03b1$-dominating sets, and we identify sharp threshold phenomena for both of them using probabilistic methods, duality arguments, and rounding techniques.\n  We first consider $\u03b1$-winning sets. A set $S$ of $k$ candidates is $\u03b1$-winning if, for every outside candidate $a \\notin S$, at least an $\u03b1$-fraction of voters rank some member of $S$ above $a$. We show a sharp threshold at \\[ \u03b1_{\\mathrm{win}}^\\star = 1 - \\frac{1}{k}. \\] Specifically, an $\u03b1$-winning set of size $k$ exists with high probability when $\u03b1< \u03b1_{\\mathrm{win}}^\\star$, and is unlikely to exist when $\u03b1> \u03b1_{\\mathrm{win}}^\\star$.\n  We then study the stronger notion of $\u03b1$-dominating sets. A set $S$ of $k$ candidates is $\u03b1$-dominating if, for every outside candidate $a \\notin S$, there exists a single committee member $b \\in S$ such that at least an $\u03b1$-fraction of voters prefer $b$ to $a$. Here we establish an analogous sharp threshold at \\[ \u03b1_{\\mathrm{dom}}^\\star = \\frac{1}{2} - \\frac{1}{2k}. \\] As a corollary, our analysis yields an impossibility result for $\u03b1$-dominating sets: for every $k$ and every $\u03b1> \u03b1_{\\mathrm{dom}}^\\star = 1 / 2 - 1 / (2k)$, there exist preference profiles that admit no $\u03b1$-dominating set of size $k$. This corollary improves the best previously known bounds for all $k \\geq 2$.",
    "domain": "math",
    "subdomain": "math.CO",
    "arxiv_id": "2602.04815v1"
  },
  {
    "id": 101,
    "title": "RareCollab -- An Agentic System Diagnosing Mendelian Disorders with Integrated Phenotypic and Molecular Evidence",
    "abstract": "Millions of children worldwide are affected by severe rare Mendelian disorders, yet exome and genome sequencing still fail to provide a definitive molecular diagnosis for a large fraction of patients, prolonging the diagnostic odyssey. Bridging this gap increasingly requires transitioning from DNA-only interpretation to multi-modal diagnostic reasoning that combines genomic data, transcriptomic sequencing (RNA-seq), and phenotype information; however, computational frameworks that coherently integrate these signals remain limited. Here we present RareCollab, an agentic diagnostic framework that pairs a stable quantitative Diagnostic Engine with Large Language Model (LLM)-based specialist modules that produce high-resolution, interpretable assessments from transcriptomic signals, phenotypes, variant databases, and the literature to prioritize potential diagnostic variants. In a rigorously curated benchmark of Undiagnosed Diseases Network (UDN) patients with paired genomic and transcriptomic data, RareCollab achieved 77% top-5 diagnostic accuracy and improved top-1 to top-5 accuracy by ~20% over widely used variant-prioritization approaches. RareCollab illustrates how modular artificial intelligence (AI) can operationalize multi-modal evidence for accurate, scalable rare disease diagnosis, offering a promising path toward reducing the diagnostic odyssey for affected families.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2602.04058v1"
  },
  {
    "id": 105,
    "title": "DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis",
    "abstract": "Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.\n  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2602.01839v1"
  },
  {
    "id": 107,
    "title": "Classification of SARS-CoV-2 Variants through The Epistatical Circos Plots with Convolutional Neural Networks",
    "abstract": "The COVID-19 pandemic has profoundly affected global health, driven by the remarkable transmissibility and mutational adaptability of the SARS-CoV-2 virus. Five major variants of concern, Alpha, Beta, Gamma, Delta, and Omicron, have been identified. By August 2022, over 12.95 million full-length SARS-CoV-2 genome sequences had been deposited in the Global Initiative on Sharing Avian Influenza Data (GISAID) database, offering an unprecedented opportunity to investigate viral evolution and epistatic interactions. Recent advances in epistatic inference, exemplified by Direct Coupling Analysis (DCA) (Zeng et al., Phys. Rev. E, 2022), have generated numerous Circos plots illustrating genetic inter-dependencies. In this study, we constructed a dataset of 1,984 Circos plots and developed a convolutional neural network (CNN) framework to classify and identify the corresponding genomic variants. The CNN effectively captured complex epistatic features, achieving an accuracy of 99.26\\%. These findings demonstrate that CNN-based models can serve as powerful tools for exploring higher-order genetic dependencies, providing deeper insights into the evolutionary dynamics and adaptive mechanisms of SARS-CoV-2.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.22866v1"
  },
  {
    "id": 108,
    "title": "Accelerating De Novo Genome Assembly via Quantum-Assisted Graph Optimization with Bitstring Recovery",
    "abstract": "Genome sequencing is essential to decode genetic information, identify organisms, understand diseases and advance personalized medicine. A critical step in any genome sequencing technique is genome assembly. However, de novo genome assembly, which involves constructing an entire genome sequence from scratch without a reference genome, presents significant challenges due to its high computational complexity, affecting both time and accuracy. In this study, we propose a hybrid approach utilizing a quantum computing-based optimization algorithm integrated with classical pre-processing to expedite the genome assembly process. Specifically, we present a method to solve the Hamiltonian and Eulerian paths within the genome assembly graph using gate-based quantum computing through a Higher-Order Binary Optimization (HOBO) formulation with the Variational Quantum Eigensolver algorithm (VQE), in addition to a novel bitstring recovery mechanism to improve optimizer traversal of the solution space. A comparative analysis with classical optimization techniques was performed to assess the effectiveness of our quantum-based approach in genome assembly. The results indicate that, as quantum hardware continues to evolve and noise levels diminish, our formulation holds a significant potential to accelerate genome sequencing by offering faster and more accurate solutions to the complex challenges in genomic research.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2602.00156v1"
  },
  {
    "id": 109,
    "title": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
    "abstract": "Current genomic foundation models (GFMs) rely on extensive neural computation to implicitly approximate conserved biological motifs from single-nucleotide inputs. We propose Gengram, a conditional memory module that introduces an explicit and highly efficient lookup primitive for multi-base motifs via a genomic-specific hashing scheme, establishing genomic \"syntax\". Integrated into the backbone of state-of-the-art GFMs, Gengram achieves substantial gains (up to 14%) across several functional genomics tasks. The module demonstrates robust architectural generalization, while further inspection of Gengram's latent space reveals the emergence of meaningful representations that align closely with fundamental biological knowledge. By establishing structured motif memory as a modeling primitive, Gengram simultaneously boosts empirical performance and mechanistic interpretability, providing a scalable and biology-aligned pathway for the next generation of GFMs. The code is available at https://github.com/zhejianglab/Genos, and the model checkpoint is available at https://huggingface.co/ZhejiangLab/Gengram.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.22203v1"
  },
  {
    "id": 118,
    "title": "SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model",
    "abstract": "Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.15504v1"
  },
  {
    "id": 122,
    "title": "engGNN: A Dual-Graph Neural Network for Omics-Based Disease Classification and Feature Selection",
    "abstract": "Omics data, such as transcriptomics, proteomics, and metabolomics, provide critical insights into disease mechanisms and clinical outcomes. However, their high dimensionality, small sample sizes, and intricate biological networks pose major challenges for reliable prediction and meaningful interpretation. Graph Neural Networks (GNNs) offer a promising way to integrate prior knowledge by encoding feature relationships as graphs. Yet, existing methods typically rely solely on either an externally curated feature graph or a data-driven generated one, which limits their ability to capture complementary information. To address this, we propose the external and generated Graph Neural Network (engGNN), a dual-graph framework that jointly leverages both external known biological networks and data-driven generated graphs. Specifically, engGNN constructs a biologically informed undirected feature graph from established network databases and complements it with a directed feature graph derived from tree-ensemble models. This dual-graph design produces more comprehensive embeddings, thereby improving predictive performance and interpretability. Through extensive simulations and real-world applications to gene expression data, engGNN consistently outperforms state-of-the-art baselines. Beyond classification, engGNN provides interpretable feature importance scores that facilitate biologically meaningful discoveries, such as pathway enrichment analysis. Taken together, these results highlight engGNN as a robust, flexible, and interpretable framework for disease classification and biomarker discovery in high-dimensional omics contexts.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.14536v1"
  },
  {
    "id": 124,
    "title": "Multimodal Spatial Omics: From Data Acquisition to Computational Integration",
    "abstract": "Recent developments in spatial omics technologies have enabled the generation of high dimensional molecular data, such as transcriptomes, proteomes, and epigenomes, within their spatial tissue context, either through coprofiling on the same slice or through serial tissue sections. These datasets, which are often complemented by images, have given rise to multimodal frameworks that capture both the cellular and architectural complexity of tissues across multiple molecular layers. Integration in such multimodal data poses significant computational challenges due to differences in scale, resolution, and data modality. In this review, we present a comprehensive overview of computational methods developed to integrate multimodal spatial omics and imaging datasets. We highlight key algorithmic principles underlying these methods, ranging from probabilistic to the latest deep learning approaches.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.12381v1"
  },
  {
    "id": 125,
    "title": "GP-DHT: A Dual-Head Transformer with Contras-tive Learning for Predicting Gene Regulatory Rela-tionships across Species from Single-Cell Data",
    "abstract": "Gene regulatory networks (GRNs) are essential for understanding cell fate decisions and disease mechanisms, yet cross-species GRN inference from single-cell RNA-seq data remains challenging due to noise, sparsity, and cross-species distribution shifts. We propose GP-DHT (GenePair DualHeadTransformer), a cross-species single-cell GRN inference framework that models genes and cells in a heterogeneous graph with multi-level expression relations and learns structured regulatory representations via multi-relational graph attention. A dual-head Transformer further captures local gene pair regulatory dependencies and global cross-cell interaction patterns. To improve robustness under sparse and cross-species settings, GP-DHT introduces gene pair level supervised contrastive learning. Experiments on seven BEELINE benchmark datasets show consistent gains over representative baselines, improving AUROC and AUPRC by approximately 5 to 7 percent on most datasets. GP-DHT also recovers known regulatory modules and helps distinguish conserved from species-specific regulations.",
    "domain": "q-bio",
    "subdomain": "q-bio.GN",
    "arxiv_id": "2601.10995v1"
  }
]