{
  "metadata": {
    "session": 53,
    "total_papers": 40,
    "min_score": 7,
    "max_score": 7
  },
  "papers": [
    {
      "paper_id": 117,
      "title": "PhageMind: Generalized Strain-level Phage Host Range Prediction via Meta-learning",
      "abstract": "Bacteriophages (phages) are key regulators of bacterial populations and hold great promise for applications such as phage therapy, biocontrol, and industrial fermentation. The success of these applications depends on accurately determining phage host range, which is often specific at the strain level rather than the species level. However, existing computational approaches face major limitations: many rely on genus-specific features that do not generalize across taxa, while others require large amounts of training data that are unavailable for most bacterial lineages. These challenges create a critical need for methods that can accurately predict strain-level phage-host interactions across diverse bacterial genera, particularly under data-limited conditions. We present PhageMind, a learning framework designed to address this challenge by enabling efficient transfer of knowledge across bacterial genera. PhageMind is trained to identify shared principles of phage-bacterium interactions from well-studied systems and to rapidly adapt these principles to new genera using only a small number of known interactions. To reflect the biological basis of infection, we represent phage-host relationships using a knowledge graph that explicitly incorporates phage tail fiber proteins and bacterial O-antigen biosynthesis gene clusters, and we use this representation to guide interaction prediction. Across four bacterial genera (Escherichia, Klebsiella, Vibrio, and Alteromonas), PhageMind achieves high prediction accuracy and shows strong adaptability to new lineages. In particular, in leave-one-genus-out evaluations, the model maintains robust performance when only limited reference data are available, demonstrating its potential as a scalable and practical tool for studying phage-host interactions across the global phageome.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 140,
      "title": "Non-Stationary Inventory Control with Lead Times",
      "abstract": "We study non-stationary single-item, periodic-review inventory control problems in which the demand distribution is unknown and may change over time. We analyze how demand non-stationarity affects learning performance across inventory models, including systems with demand backlogging or lost-sales, both with and without lead times. For each setting, we propose an adaptive online algorithm that optimizes over the class of base-stock policies and establish performance guarantees in terms of dynamic regret relative to the optimal base-stock policy at each time step. Our results reveal a sharp separation across inventory models. In backlogging systems and lost-sales models with zero lead time, we show that it is possible to adapt to demand changes without incurring additional performance loss in stationary environments, even without prior knowledge of the demand distributions or the number of demand shifts. In contrast, for lost-sales systems with positive lead times, we establish weaker guarantees that reflect fundamental limitations imposed by delayed replenishment in combination with censored feedback. Our algorithms leverage the convexity and one-sided feedback structure of inventory costs to enable counterfactual policy evaluation despite demand censoring. We complement the theoretical analysis with simulation results showing that our methods significantly outperform existing benchmarks.",
      "domain": "stat",
      "score": 7
    },
    {
      "paper_id": 145,
      "title": "Muon in Associative Memory Learning: Training Dynamics and Scaling Laws",
      "abstract": "Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.",
      "domain": "stat",
      "score": 7
    },
    {
      "paper_id": 148,
      "title": "Piecewise Deterministic Markov Processes for Bayesian Inference of PDE Coefficients",
      "abstract": "We develop a general framework for piecewise deterministic Markov process (PDMP) samplers that enables efficient Bayesian inference in non-linear inverse problems with expensive likelihoods. The key ingredient is a surrogate-assisted thinning scheme in which a surrogate model provides a proposal event rate and a robust correction mechanism enforces an upper bound on the true rate by dynamically adjusting an additive offset whenever violations are detected. This construction is agnostic to the choice of surrogate and PDMP, and we demonstrate it for the Zig-Zag sampler and the Bouncy particle sampler with constant, Laplace, and Gaussian process (GP) surrogates, including gradient-informed and adaptively refined GP variants. As a representative application, we consider Bayesian inference of a spatially varying Young's modulus in a one-dimensional linear elasticity problem. Across dimensions, PDMP samplers equipped with GP-based surrogates achieve substantially higher accuracy and effective sample size per forward model evaluation than Random Walk Metropolis algorithm and the No-U-Turn sampler. The Bouncy particle sampler exhibits the most favorable overall efficiency and scaling, illustrating the potential of the proposed PDMP framework beyond this particular setting.",
      "domain": "stat",
      "score": 7
    },
    {
      "paper_id": 151,
      "title": "Auditory frequency analysis as an active dissipative process",
      "abstract": "An active dissipative process organizes auditory frequency analysis in the mammalian cochlea. A minimal active beam model reveals that a spatially varying viscous coupling operator, $\\partial_{xx}\u03ba\\partial_{xx}$, generates dissipative forces with wave--like propagation. Local energy injection and spatial redistribution compete to govern the dynamics. This balance enables the quantitative reproduction of four key features: sharp tuning, high gain, compression, and spontaneous otoacoustic emissions. Hearing thereby belongs to a broad class of nonequilibrium pattern-forming systems.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 159,
      "title": "Thermodynamic cost-controllability tradeoff in metabolic currency coupling",
      "abstract": "Cellular metabolism is globally regulated by various currency metabolites such as ATP, GTP, and NAD(P)H. These metabolites cycle between charged (high-energy) and uncharged (low-energy) states to mediate energy transfer. While distinct currency metabolites are associated with different metabolic functions, their charged and uncharged forms are generally interchangeable via biochemical reactions such as ${\\rm ATP{\\,+\\,}GDP{\\,\\rightleftharpoons\\,}ADP{\\,+\\,}GTP}$ and $\\rm NADP^+{\\,+\\,}NADH{\\,\\rightleftharpoons\\,}NADPH{\\,+\\,}NAD^+ $. Thus, their energetic states are generally coupled and influence each other, which would hinder the independent regulation of different currency metabolites. Despite the extensive knowledge of the molecular biology of individual currency metabolites, it remains poorly understood how the coordination of various coupled currency metabolites shapes metabolic regulation, efficiency, and ultimately the evolution of organisms. Here, we present a minimal theoretical model of metabolic currency coupling and reveal a fundamental tradeoff relationship between metabolic controllability and thermodynamic cost: increasing the capacity to independently regulate multiple currency metabolites generally requires comparable abundances of those metabolites, which in turn incurs a higher entropy production rate. The tradeoff suggests that in complex environments, organisms evolutionarily favor an equal abundance of currency metabolites to enhance metabolic controllability at the expense of a higher thermodynamic cost; conversely, in simple environments, organisms evolve to have imbalanced amounts of them to reduce heat dissipation. These considerations also offer a hypothesis regarding evolutionary trends in nucleotide-pool balance and genomic GC content.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 171,
      "title": "Plasticity, hysteresis, and recovery mechanisms in spider silk fibers",
      "abstract": "Spider silk is a remarkable biomaterial with exceptional stiffness, strength, and toughness stemming from a unique microstructure. While recent studies show that silk fibers exhibit plasticity, hysteresis, and recovery under cyclic loading, the underlying microstructural mechanisms are not yet fully understood. In this work, we propose a mechanism explaining the loading-unloading-relaxation response through microstructural evolution: initial loading distorts intermolecular bonds, resulting in a linear elastic regime. Upon reaching the yield stress, these bonds dissociate and the external load is transferred to the polypeptide chains, which deform entropically to allow large deformations. Unloading is driven by entropic shortening until a traction free state with residual stretch is achieved. Subsequently, the fiber recovers as chains reorganize and bonds reform, locking the microstructure into a new stable equilibrium that increases stiffness in subsequent cycles. Following these mechanisms, we develop a microscopically motivated, energy-based model that captures the macroscopic response of silk fibers under cyclic loading. The response is decoupled into two parallel networks: (1) an elasto-plastic network of inter- and intramolecular bonds governing the initial stiffness and yield stress, and (2) an elastic network of entropic chains that enable large deformations. The model is validated against experimental data from Argiope bruennichi dragline silk. The findings from this work are three-fold: (1) explaining the mechanisms that govern hysteresis and recovery and linking them to microstructural evolution; (2) quantifying the recovery process of the fiber, which restores and enhances mechanical properties; and (3) establishing a predictive foundation for engineering synthetic fibers with customized properties.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 232,
      "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
      "abstract": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 239,
      "title": "A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders",
      "abstract": "Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 308,
      "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
      "abstract": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 332,
      "title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation",
      "abstract": "Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted\" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling\" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 336,
      "title": "Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation",
      "abstract": "Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 340,
      "title": "Contour Refinement using Discrete Diffusion in Low Data Regime",
      "abstract": "Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 345,
      "title": "Higher-order adaptive behaviors outperform pairwise strategies in mitigating contagion dynamics",
      "abstract": "When exposed to a contagion phenomenon, individuals may respond to the perceived risk of infection by adopting behavioral changes, aiming to reduce their exposure or their risk of infecting others. The social cost of such adaptive behaviors and their impact on the contagion dynamics have been investigated in pairwise networks, with binary interactions driving both contagion and risk perception. However, contagion and adaptive mechanisms can also be driven by group (higher-order) interactions. Here, we consider several adaptive behaviors triggered by awareness of risk perceived through higher-order and pairwise interactions, and we compare their impact on pairwise and higher-order contagion processes. By numerical simulations and a mean-field analytic approach, we show that adaptive behaviors driven by higher-order information are more effective in limiting the spread of a contagion, than similar mechanisms based on pairwise information. Meanwhile, they also entail a lower social cost, measured as the reduction of the intensity of interactions in the population. Indeed, adaptive mechanisms based on higher-order information lead to a heterogeneous risk perception within the population, producing a higher alert on nodes with large hyperdegree (i.e., participating in many groups), on their neighborhoods, and on large groups. This in turn prevents the spreading process to exploit the properties of these nodes and groups, which tend to drive and sustain the dynamics in the absence of adaptive behaviors.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 358,
      "title": "Emergence and co-existence of periodic and unstructured motion in future-avoiding random walks",
      "abstract": "Self-avoiding random walks on graphs can be seen as walkers interacting with their own past history. This letter considers a complementary class of dynamics: Mutual future avoiding random walks (MFARWs), where stochastically driven walkers are avoiding each others planned future trajectories. Such systems arise naturally in conceptual models of shared mobility. We show that periodic behavior emerges spontaneously in such MFARWs, and that periodic and unstructured behavior coexist, providing a first example of Chimera style behavior of non-oscillatory paths on networks. Further, we analytically describe and predict the onset of structure. We find that the phase transition from unstructured to periodic behavior is driven by a novel mechanism of self-amplifying coupling to the periodic components of the stochastic drivers of the system. In the context of shared mobility applications, these Chimera states imply a regime of naturally stable co-existence between flexible and line-based public transport.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 365,
      "title": "Social Catalysts, Not Moral Agents: The Illusion of Alignment in LLM Societies",
      "abstract": "The rapid evolution of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems where collective cooperation is often threatened by the \"Tragedy of the Commons.\" This study investigates the effectiveness of Anchoring Agents--pre-programmed altruistic entities--in fostering cooperation within a Public Goods Game (PGG). Using a full factorial design across three state-of-the-art LLMs, we analyzed both behavioral outcomes and internal reasoning chains. While Anchoring Agents successfully boosted local cooperation rates, cognitive decomposition and transfer tests revealed that this effect was driven by strategic compliance and cognitive offloading rather than genuine norm internalization. Notably, most agents reverted to self-interest in new environments, and advanced models like GPT-4.1 exhibited a \"Chameleon Effect,\" masking strategic defection under public scrutiny. These findings highlight a critical gap between behavioral modification and authentic value alignment in artificial societies.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 394,
      "title": "Graph-Based Audits for Meek Single Transferable Vote Elections",
      "abstract": "In the context of election security, a Risk-Limiting Audit (RLA) is a statistical framework that uses a minimal partial recount of the ballots to guarantee that the results of the election were correctly reported. A generalized RLA framework has remained elusive for algorithmic election rules such as the Single Transferable Vote (STV) rule, because of the dependence of these rules on the chronology of eliminations and elections leading to the outcome of the election. This paper proposes a new graph-based approach to audit these algorithmic election rules, by considering the space of all possible sequences of elections and eliminations. If we fix a subgraph of this universal space ahead of the audit, a sufficient strategy is to verify statistically that the true election sequence does not leave the fixed subgraph. This makes for a flexible framework to audit these elections in a chronology-agnostic way.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 406,
      "title": "Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention",
      "abstract": "Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 409,
      "title": "Evolutionary Mapping of Neural Networks to Spatial Accelerators",
      "abstract": "Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 411,
      "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization",
      "abstract": "The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework's efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 416,
      "title": "Equilibrium Propagation for Non-Conservative Systems",
      "abstract": "Equilibrium Propagation (EP) is a physics-inspired learning algorithm that uses stationary states of a dynamical system both for inference and learning. In its original formulation it is limited to conservative systems, $\\textit{i.e.}$ to dynamics which derive from an energy function. Given their importance in applications, it is important to extend EP to nonconservative systems, $\\textit{i.e.}$ systems with non-reciprocal interactions. Previous attempts to generalize EP to such systems failed to compute the exact gradient of the cost function. Here we propose a framework that extends EP to arbitrary nonconservative systems, including feedforward networks. We keep the key property of equilibrium propagation, namely the use of stationary states both for inference and learning. However, we modify the dynamics in the learning phase by a term proportional to the non-reciprocal part of the interaction so as to obtain the exact gradient of the cost function. This algorithm can also be derived using a variational formulation that generates the learning dynamics through an energy function defined over an augmented state space. Numerical experiments using the MNIST database show that this algorithm achieves better performance and learns faster than previous proposals.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 418,
      "title": "Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery",
      "abstract": "Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erd\u0151s-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 426,
      "title": "Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL",
      "abstract": "Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 451,
      "title": "Optimal Harvesting in Stream Networks: Maximizing Biomass and Yield",
      "abstract": "In this study, we develop a metapopulation model framework to identify optimal harvesting strategies for a population in a stream network. We consider two distinct optimization objectives: maximization of total biomass and maximization of total yield, under the constraint of a fixed total harvesting effort. We examine in detail the special case of a two-patch network and fully characterize the optimal strategies for each objective. We show that when the population growth rate exceeds a critical threshold, a single harvesting strategy can simultaneously maximize both objectives. For general $n$-patch networks with homogeneous growth rates across patches, we focus on the regime of large growth rates and demonstrate that the optimal harvesting strategy selects patches according to their intraspecific competition rates and an effective net flow metric determined by network connectivity parameters.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 452,
      "title": "Uncertainty in Island-based Ecosystem Services and Climate Change",
      "abstract": "Small and medium-sized islands are acutely exposed to climate change and ecosystem degradation, yet the extent to which uncertainty is systematically addressed in scientific assessments of their ecosystem services remains poorly understood. This study revisits 226 peer-reviewed articles drawn from two global systematic reviews on island ecosystem services and climate change, applying a structured post hoc analysis to evaluate how uncertainty is treated across methods, service categories, ecosystem realms, and decision contexts. Studies were classified according to whether uncertainty was explicitly analysed, just mentioned, or ignored. Only 30 percent of studies incorporated uncertainty explicitly, while more than half did not address it at all. Scenario-based approaches dominated uncertainty assessment, whereas probabilistic and ensemble-based frameworks remained limited. Cultural ecosystem services and extreme climate impacts exhibited the lowest levels of uncertainty integration, and few studies connected uncertainty treatment to policy relevant decision frameworks. Weak or absent treatment of uncertainty emerges as a structural challenge in island systems, where narrow ecological thresholds, strong land-sea coupling, limited spatial buffers, and reduced institutional redundancy amplify the consequences of decision-making under incomplete knowledge. Systematic mapping of how uncertainty is framed, operationalised, or neglected reveals persistent methodological and conceptual gaps and informs concrete directions for strengthening uncertainty integration in future island-focused ecosystem service and climate assessments. Embedding uncertainty more robustly into modelling practices, participatory processes, and policy tools is essential for enhancing scientific credibility, governance relevance, and adaptive capacity in insular socio-ecological systems.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 464,
      "title": "Impact of behavioral heterogeneity on epidemic outcome and its mapping into effective network topologies",
      "abstract": "Human behavior plays a critical role in shaping epidemic trajectories. During health crises, people respond in diverse ways in terms of self-protection and adherence to recommended measures, largely reflecting differences in how individuals assess risk. This behavioral variability induces effective heterogeneity into key epidemic parameters, such as infectivity and susceptibility. We introduce a minimal extension of the susceptible-infected-removed~(SIR) model, denoted HeSIR, that captures these effects through a simple bimodal scheme, where individuals may have higher or lower transmission--related traits. We derive a closed-form expression for the epidemic threshold in terms of the model parameters, and the network's degree distribution and homophily, defined as the tendency of like--risk individuals to preferentially interact. We identify a resurgence regime just beyond the classical threshold, where the number of infected individuals may initially decline before surging into large-scale transmission. Through simulations on homogeneous and heterogeneous network topologies we corroborate the analytical results and highlight how variations in susceptibility and infectivity influence the epidemic dynamics. We further show that, under suitable assumptions, the HeSIR model maps onto a standard SIR process on an appropriately modified contact network, providing a unified interpretation in terms of structural connectivity. Our findings quantify the effect of heterogeneous behavioral responses, especially in the presence of homophily, and caution against underestimating epidemic potential in fragmented populations, which may undermine timely containment efforts. The results also extend to heterogeneity arising from biological or other non-behavioral sources.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 467,
      "title": "The quenched structured coalescent for diploid population models on finite graphs with large migrations and uneven offspring distributions",
      "abstract": "In this work we describe a new model for the evolution of a diploid structured population backwards in time that allows for large migrations and uneven offspring distributions. The model generalizes both the mean-field model of Birkner et al. [\\textit{Electron. J. Probab.} 23: 1-44 (2018)] and the haploid structured model of M\u00f6hle [\\textit{Theor. Popul. Biol.} 2024 Apr:156:103-116]. We show convergence, with mild conditions on the joint distribution of offspring frequencies and migrations, of gene genealogies conditional on the pedigree to a time-inhomogeneous coalescent process driven by a Poisson point process $\u03a8$ that records the timing and scale of large migrations and uneven offspring distributions. This quenched scaling limit demonstrates a significant difference in the predictions of the classical annealed theory of structured coalescent processes. In particular, the annealed and quenched scaling limits coincide if and only if these large migrations and uneven offspring distributions are absent. The proof proceeds by the method of moments and utilizes coupling techniques from the theory of random walks in random environments. Several examples are given and their quenched scaling limits established.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 485,
      "title": "Fully Automated Adaptive Parameter Selection for 3-D High-order Nystr\u00f6m Boundary Integral Equation Methods",
      "abstract": "We present an adaptive Chebyshev-based Boundary Integral Equation (CBIE) solver for electromagnetic scattering from smooth perfect electric conductor (PEC) objects. The proposed approach eliminates manual parameter tuning by introducing (i) a unified adaptive quadrature strategy for automatic selection of the near-singular interaction distance and (ii) an adaptive computation of all self- and near-singular precomputation integrals to a prescribed accuracy using Gauss-Kronrod (h-adaptive) or Clenshaw-Curtis (p-adaptive) rules and singularity-resolving changes of variables. Both h-adaptive and p-adaptive schemes are explored within this framework, ensuring high-order accuracy and robustness across a broad range of geometries without loss of efficiency. Numerical results for canonical and complex CAD geometries demonstrate that the adaptive solver achieves accuracy and convergence rates comparable to optimally tuned fixed-grid CBIE implementations, while offering automation and scalability to electrically large, geometrically complex problems.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 489,
      "title": "Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS",
      "abstract": "State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 514,
      "title": "Phase Transitions in Unsupervised Feature Selection",
      "abstract": "Identifying minimal and informative feature sets is a central challenge in data analysis, particularly when few data points are available. Here we present a theoretical analysis of an unsupervised feature selection pipeline based on the Differentiable Information Imbalance (DII). We consider the specific case of structural and physico-chemical features describing a set of proteins. We show that if one considers the features as coordinates of a (hypothetical) statistical physics model, this model undergoes a phase transition as a function of the number of retained features. For physico-chemical descriptors, this transition is between a glass-like phase when the features are few and a liquid-like phase. The glass-like phase exhibits bimodal order-parameter distributions and Binder cumulant minima. In contrast, for structural descriptors the transition is less sharp. Remarkably, for physico-chemical descriptors the critical number of features identified from the DII coincides with the saturation of downstream binary classification performance. These results provide a principled, unsupervised criterion for minimal feature sets in protein classification and reveal distinct mechanisms of criticality across different feature types.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 516,
      "title": "Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation",
      "abstract": "Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 517,
      "title": "Minimal-Action Discrete Schr\u00f6dinger Bridge Matching for Peptide Sequence Design",
      "abstract": "Generative modeling of peptide sequences requires navigating a discrete and highly constrained space in which many intermediate states are chemically implausible or unstable. Existing discrete diffusion and flow-based methods rely on reversing fixed corruption processes or following prescribed probability paths, which can force generation through low-likelihood regions and require countless sampling steps. We introduce Minimal-action discrete Schr\u00f6dinger Bridge Matching (MadSBM), a rate-based generative framework for peptide design that formulates generation as a controlled continuous-time Markov process on the amino-acid edit graph. To yield probability trajectories that remain near high-likelihood sequence neighborhoods throughout generation, MadSBM 1) defines generation relative to a biologically informed reference process derived from pre-trained protein language model logits and 2) learns a time-dependent control field that biases transition rates to produce low-action transport paths from a masked prior to the data distribution. We finally introduce guidance to the MadSBM sampling procedure towards a specific functional objective, expanding the design space of therapeutic peptides; to our knowledge, this represents the first-ever application of discrete classifier guidance to Schr\u00f6dinger bridge-based generative models.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 524,
      "title": "A novel scalable high performance diffusion solver for multiscale cell simulations",
      "abstract": "Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 526,
      "title": "Modeling tumor progression in heterogeneous microenvironments: A cellular automata approach",
      "abstract": "Understanding how microenvironmental heterogeneity influences tumor progression is essential for advancing both cancer biology and therapeutic strategies. In this study, we develop a cellular automata (CA) model to simulate tumor growth under varying microenvironmental conditions and genetic mutation rates, addressing a gap in existing studies that rarely integrate these two factors to explain tumor dynamics. The model explicitly incorporates the cellular heterogeneity of stem and non-stem cells, dynamic cell-cell interactions, and tumor-microenvironment crosstalk. Using computational simulations, we examine the synergistic effects of gene mutation rate, initial tumor burden, and microenvironmental state on tumor progression. Our results demonstrate that lowering the mutation rate significantly mitigates tumor expansion and preserves microenvironmental integrity. Interestingly, the initial tumor burden has a limited impact, whereas the initial condition of the microenvironment critically shapes tumor dynamics. A supportive microenvironment promotes proliferation and spatial invasion, while inhibitory conditions suppress tumor growth. These findings highlight the key role of microenvironmental modulation in tumor evolution and provide computational insights that may inform more effective cancer therapies.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 552,
      "title": "Spatiotemporal Topological Phase Transition in non-Hermitian Photonic System",
      "abstract": "While energy band topology in spatial photonic crystals (PCs) and momentum-band topology in temporal crystals have each served as powerful probes of topological phases in their respective domains, their unification in a static platform remains unexplored. In this Letter, we bridge this gap by introducing a waveguide assisted non-Hermitian SSH model, in which controlled tuning of loss and coupling drives PT-symmetry breaking and enables a continuous transition between energy- and momentum-gap regimes. This allows us to construct a complete spatiotemporal topological phase diagram in a unified parameter space. By mapping this phase diagram onto a spatially graded PC, we experimentally observe multiple Bloch momentum-band gaps and a continuous spatiotemporal topological transition via translating across the static sample, enabling real-time control over the evolution pathway of the band topology. Our work creates a versatile, bias-free platform for exploring synthetic spacetime physics and opens new avenues for controlling light via non-Hermitian band engineering.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 570,
      "title": "Semiclassical Structure of the Advection--Diffusion Spectrum in Mixed Phase Spaces",
      "abstract": "We examine the spectral structure of the two-dimensional advection-diffusion operator in flows with mixed phase space at very large Peclet number. Using Fourier discretization combined with symmetry reduction and Krylov-Arnoldi methods, we compute on the order of one hundred leading eigenpairs reliably in the asymptotic, weak-diffusion regime. While the principal eigenvalue is asymptotically diffusive and localized on the largest regular region, the broader spectrum exhibits a rich organization controlled by local Lagrangian phase-space geometry. In particular, exponential mixing in chaotic regions rapidly suppresses correlations, whereas algebraic mixing in integrable regions generates long-lived coherent structures that dominate the slow and intermediate parts of the spectrum. We identify three distinct classes of eigenmodes: advective modes associated with transport on invariant tori, diffusive modes and, within the duffusive branch, tunneling modes arising from weak coupling between dynamically separated regular regions. Drawing on a semiclassical analogy, we assign quantum-number-like labels to these families and predict the appearance, scaling, and ordering of their sub-spectra directly from the Hamiltonian phase-space structure. The coexistence of these families implies that no uniform control of the spectral gap exists across the full spectrum: although the slowest mode is diffusive, arbitrarily small gaps arise between competing families at higher mode numbers. As a result, finite-time advection-diffusion dynamics is generically governed by persistent modal competition rather than single-mode dominance, even at asymptotically large Peclet number.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 591,
      "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
      "abstract": "The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 594,
      "title": "Entanglement improves coordination in distributed systems",
      "abstract": "Coordination in distributed systems is often hampered by communication latency, which degrades performance. Quantum entanglement offers fundamentally stronger correlations than classically achievable without communication. Crucially, these correlations manifest instantaneously upon measurement, irrespective of the physical distance separating the systems. We investigate the application of shared entanglement to a dual-work optimization problem in a distributed system comprising two servers. The system must process both a continuously available, preemptible baseline task and incoming customer requests arriving in pairs. System performance is characterized by the trade-off between baseline task throughput and customer waiting time. We present a rigorous analytical model demonstrating that when the baseline task throughput function is strictly convex, rewarding longer uninterrupted processing periods, entanglement-assisted routing strategies achieve Pareto-superior performance compared to optimal communication-free classical strategies. We prove this advantage through queueing-theoretic analysis, non-local game formulation, and computational certification of classical bounds. Our results identify distributed scheduling and coordination as a novel application domain for near-term entanglement-based quantum networks.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 602,
      "title": "Fast Private Adaptive Query Answering for Large Data Domains",
      "abstract": "Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 603,
      "title": "Time-Complexity Characterization of NIST Lightweight Cryptography Finalists",
      "abstract": "Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.",
      "domain": "cs",
      "score": 7
    }
  ]
}