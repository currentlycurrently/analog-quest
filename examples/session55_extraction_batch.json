{
  "session": 55,
  "total_papers": 50,
  "papers": [
    {
      "paper_id": 489,
      "title": "Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS",
      "abstract": "State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 524,
      "title": "A novel scalable high performance diffusion solver for multiscale cell simulations",
      "abstract": "Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 603,
      "title": "Time-Complexity Characterization of NIST Lightweight Cryptography Finalists",
      "abstract": "Lightweight cryptography is becoming essential as emerging technologies in digital identity systems and Internet of Things verification continue to demand strong cryptographic assurance on devices with limited processing power, memory, and energy resources. As these technologies move into routine use, they demand cryptographic primitives that maintain strong security and deliver predictable performance through clear theoretical models of time complexity. Although NIST's lightweight cryptography project provides empirical evaluations of the ten finalist algorithms, a unified theoretical understanding of their time-complexity behavior remains absent. This work introduces a symbolic model that decomposes each scheme into initialization, data-processing, and finalization phases, enabling formal time-complexity derivation for all ten finalists. The results clarify how design parameters shape computational scaling on constrained mobile and embedded environments. The framework provides a foundation needed to distinguish algorithmic efficiency and guides the choice of primitives capable of supporting security systems in constrained environments.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 628,
      "title": "On Path-based Marginal Cost of Heterogeneous Traffic Flow for General Networks",
      "abstract": "Path marginal cost (PMC) is a crucial component in solving path-based system-optimal dynamic traffic assignment (SO-DTA), dynamic origin-destination demand estimation (DODE), and network resilience analysis. However, accurately evaluating PMC in heterogeneous traffic conditions poses significant challenges. Previous studies often focus on homogeneous traffic flow of single vehicle class and do not well address the interactive effect of heterogeneous traffic flows and the resultant computational issues. This study proposes a novel but simple method for approximately evaluating PMC in complex heterogeneous traffic condition. The method decomposes PMC into intra-class and inter-class terms and uses conversion factor derived from heterogeneous link dynamics to explicitly model the intricate relationships between vehicle classes. Additionally, the method considers the non-differentiable issue that arises when mixed traffic flow approaches system optimum conditions. The proposed method is tested on a small corridor network with synthetic demand and a large-scale network with calibrated demand from real-world data. Results demonstrated that our method exhibits superior performance in solving bi-class SO-DTA problems, yielding lower total travel cost and capturing the multi-class flow competition at the system optimum state.",
      "domain": "math",
      "score": 7
    },
    {
      "paper_id": 632,
      "title": "Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation",
      "abstract": "Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 650,
      "title": "Stochastic hierarchical data-driven optimization: application to plasma-surface kinetics",
      "abstract": "This work introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory for the efficient calibration of physical models. Central to this method is the use of a reduced Hessian approximation, which identifies and targets the stiff parameter subspace using minimal simulation queries. This strategy enables efficient navigation of highly anisotropic landscapes, avoiding the computational burden of exhaustive sampling. To ensure rigorous inference, we integrate this approach with a probabilistic formulation that derives a principled objective loss function directly from observed data. We validate the framework by applying it to the problem of plasma-surface interactions, where accurate modelling is strictly limited by uncertainties in surface reactivity parameters and the computational cost of kinetic simulations. Comparative analysis demonstrates that our method consistently outperforms baseline optimization techniques in sample efficiency. This approach offers a general and scalable tool for optimizing models of complex reaction systems, ranging from plasma chemistry to biochemical networks.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 661,
      "title": "Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents",
      "abstract": "Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 666,
      "title": "Dual Mind World Model Inspired Network Digital Twin for Access Scheduling",
      "abstract": "Emerging networked systems such as industrial IoT and real-time cyber-physical infrastructures demand intelligent scheduling strategies capable of adapting to dynamic traffic, deadlines, and interference constraints. In this work, we present a novel Digital Twin-enabled scheduling framework inspired by Dual Mind World Model (DMWM) architecture, for learning-informed and imagination-driven network control. Unlike conventional rule-based or purely data-driven policies, the proposed DMWM combines short-horizon predictive planning with symbolic model-based rollout, enabling the scheduler to anticipate future network states and adjust transmission decisions accordingly. We implement the framework in a configurable simulation testbed and benchmark its performance against traditional heuristics and reinforcement learning baselines under varied traffic conditions. Our results show that DMWM achieves superior performance in bursty, interference-limited, and deadline-sensitive environments, while maintaining interpretability and sample efficiency. The proposed design bridges the gap between network-level reasoning and low-overhead learning, marking a step toward scalable and adaptive NDT-based network optimization.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 667,
      "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents",
      "abstract": "Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 668,
      "title": "Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration",
      "abstract": "Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an interpretability analysis that treats orchestration as an explicit, analyzable computation, enabling the decoupling of expert interaction structure, execution order, and causal attribution. We use INFORM to evaluate an orchestrator on GSM8K, HumanEval, and MMLU using a homogeneous consortium of ten instruction-tuned experts drawn from LLaMA-3.1 8B, Qwen-3 8B, and DeepSeek-R1 8B, with controlled decoding-temperature variation, and a secondary heterogeneous consortium spanning 1B-7B parameter models. Across tasks, routing dominance is a poor proxy for functional necessity. We reveal a divergence between relational importance, captured by routing mass and interaction topology, and intrinsic importance, measured via gradient-based causal attribution: frequently selected experts often act as interaction hubs with limited causal influence, while sparsely routed experts can be structurally critical. Orchestration behaviors emerge asynchronously, with expert centralization preceding stable routing confidence and expert ordering remaining non-deterministic. Targeted ablations show that masking intrinsically important experts induces disproportionate collapse in interaction structure compared to masking frequent peers, confirming that INFORM exposes causal and structural dependencies beyond accuracy metrics alone.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 669,
      "title": "On the Uncertainty of Large Language Model-Based Multi-Agent Systems",
      "abstract": "Multi-agent systems (MAS) have emerged as a prominent paradigm for leveraging large language models (LLMs) to tackle complex tasks. However, the mechanisms governing the effectiveness of MAS built upon publicly available LLMs, specifically the underlying rationales for their success or failure, remain largely unexplored. In this paper, we revisit MAS through the perspective of uncertainty, considering both intra- and inter-agent dynamics by investigating entropy transitions during problem-solving across various topologies and six benchmark tasks. By analyzing 245 features spanning token-, trajectory-, and round-level entropy, we counterintuitively find that a single agent outperforms MAS in approximately 43.3% of cases, and that uncertainty dynamics are largely determined during the first round of interaction. Furthermore, we provide three key observations: 1) Certainty Preference: reducing uncertainty at any stage for any agent is critical for guaranteeing correct solutions; 2) Base Uncertainty: base models with lower entropy during problem-solving directly benefit MAS performance; and 3) Task Awareness: entropy dynamics of MAS play varying roles across different tasks. Building on these insights, we introduce a simple yet effective algorithm, the Entropy Judger, to select solutions from MAS's pass@k results, leading to consistent accuracy improvements across all MAS configurations and tasks. Our source code is available at https://github.com/AgenticFinLab/multiagent-entropy.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 681,
      "title": "Authorship Drift: How Self-Efficacy and Trust Evolve During LLM-Assisted Writing",
      "abstract": "Large language models (LLMs) are increasingly used as collaborative partners in writing. However, this raises a critical challenge of authorship, as users and models jointly shape text across interaction turns. Understanding authorship in this context requires examining users' evolving internal states during collaboration, particularly self-efficacy and trust. Yet, the dynamics of these states and their associations with users' prompting strategies and authorship outcomes remain underexplored. We examined these dynamics through a study of 302 participants in LLM-assisted writing, capturing interaction logs and turn-by-turn self-efficacy and trust ratings. Our analysis showed that collaboration generally decreased users' self-efficacy while increasing trust. Participants who lost self-efficacy were more likely to ask the LLM to edit their work directly, whereas those who recovered self-efficacy requested more review and feedback. Furthermore, participants with stable self-efficacy showed higher actual and perceived authorship of the final text. Based on these findings, we propose design implications for understanding and supporting authorship in human-LLM collaboration.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 696,
      "title": "Improved Fluid Modeling of Space Debris Generated Ion-Acoustic Precursor Solitons",
      "abstract": "This study reexamines the excitation of ion-acoustic precursor solitons by a supersonically moving charged debris object, incorporating two previously overlooked physical factors: the dynamic charging of the debris and the impermeable nature of its surface. The influence of charging dynamics is explored using an enhanced one-dimensional fluid-Poisson model, where the source charge is treated as a dynamical variable and solved self-consistently alongside the core plasma equations. By comparing these results with prior fixed-charge models, we evaluate the effects on soliton onset and propagation, finding that charging dynamics does not hinder soliton generation or evolution. To assess the impact of the impermeability of debris surface, a two-dimensional fluid model simulates the interaction between an electrostatically biased, impenetrable object and a flowing plasma. Modeling the object as an infinite wall disconnects the upstream and downstream plasma regions, forming a sheath without solitons -- consistent with earlier fluid and particle-in-cell simulations. However, replacing the wall with a finite object enables plasma flow around it, restoring upstream-downstream connectivity and naturally generating precursor solitons.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 701,
      "title": "Collisionless Larmor Coupling and Blob Formation in a Laser-Plasma Expanding into a Magnetized Ambient Plasma",
      "abstract": "Collisionless Larmor coupling is a fundamental process in space and astrophysical plasmas that enables momentum transfer between an expanding plasma and a magnetized ambient medium. In this paper, we report on the laboratory experimental study of Larmor coupling leading to the formation of a plasma blob associated with a laser-driven, super-Alfv\u00e9nic plasma flow on the Large Plasma Device at the University of California, Los Angeles. The high-repetition rate enables systematic spatial and temporal scans of the plasma evolution using Doppler spectroscopy, as well as measurements of the magnetic field, electrostatic field, and self-emission of both debris and ambient ions using filtered imaging. We observe the self-focusing of the laser-produced plasma and the formation of a secondary diamagnetic cavity associated with a blob composed of background ions. Doppler spectroscopy reveals the transverse velocity distribution of the background ions, providing direct evidence of ion energization via Larmor coupling. The systematic spatial and temporal scans enabled by the high-repetition rate experiment allow for a detailed characterization of the ion dynamics. These experimental observations are supported by numerical simulations that provide more insight into the kinetic-scale physics associated with blob formation as well as the role of the ambient plasma density.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 724,
      "title": "From the Hallmarks of Cancer to the Survival System: Integration and Paradigmatic Reconstruction of Classical Oncology Theories by the Tumor Existential Crisis-Driven Survival (ECDS) Theory",
      "abstract": "Malignant tumors are defined by extraordinarily intricate pathogenic mechanisms, yet classical oncological theories remain fragmented in an archipelagic state-lacking a unifying framework to integrate the processes of tumor initiation, progression, metastasis, and therapeutic resistance. This theoretical disjuncture constrains the efficacy of the confront-and-eradicate paradigm, culminating in limited therapeutic breakthroughs, recurrent drug resistance, and substantial host toxicity. We herein propose the ECDS (Tumor Existential Crisis-Driven Survival) theory, anchored in a central mechanistic axis: impairment of Existential Stability elicits compensatory hyperactivation of Survival Capacity. The ECDS framework establishes three foundational constructs (Existential Stability, Survival Capacity, and Existence Threshold) alongside three core principles, integrating pivotal theories of tumorigenesis. It furnishes a systematic account of the dynamic coupling between augmented survival capacity and declining existential stability throughout tumor evolution; reinterprets the fundamental nature and temporal hierarchy of the fourteen cancer hallmarks; clarifies the redundancy of survival pathways in tumor heterogeneity; and elucidates the adaptive underpinnings of therapeutic resistance. As a holistic integrative model, ECDS offers transformative conceptual insights for oncology by reframing tumors not merely as abnormally-proliferating cell aggregates but as biological subsystems driven into survival mode by eroding existential stability. This challenges the prevailing dogma of tumors as intrinsically aggressive entities, revealing their core traits of intrinsic vulnerability and passive adaptation, while laying a meta-theoretical foundation for reorienting cancer study and management toward threshold-regulated dynamic adaptation.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 726,
      "title": "Mechanics of axis formation in $\\textit{Hydra}$",
      "abstract": "The emergence of a body axis is a fundamental step in the development of multicellular organisms. In simple systems such as $\\textit{Hydra}$, growing evidence suggests that mechanical forces generated by collective cellular activity play a central role in this process. Here, we explore a physical mechanism for axis formation based on the coupling between active stresses and tissue elasticity. We analyse the elastic deformation induced by activity-generated stresses and show that, owing to the spherical topology of the tissue, forces globally condense toward configurations in which both elastic strain and nematic defect localise at opposite poles. These mechanically selected states define either a polar or apolar head-food axis. To characterize the condensed regime, we introduce a compact parametrization of of the active force and flux distributions, enabling analytical predictions and direct comparison with experiments. Using this framework, we calculate experimentally relevant observables, including areal strain, lateral pressure, and normal displacements during muscular contraction, as well as the detailed structure of topological defect complexes in head and foot regions. Together, our results identify a mechanical route by which active tissues can spontaneously break symmetry at the organismal scale, suggesting a general physical principle underlying body-axis specification during morphogenesis.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 740,
      "title": "Full-Covariance Chemical Langevin Predator--Prey Diffusion with Absorbing Boundaries",
      "abstract": "Many stochastic Rosenzweig--MacArthur predator--prey models inject ad hoc independent (diagonal) noise and therefore cannot encode the event-level coupling created by predation and biomass conversion. We derive an absorbed, fully mechanistic diffusion approximation and its extinction structure from a continuous-time Markov chain on $\\mathbb N_0^2$ with four reaction channels: prey birth, prey competition death, predator death, and a coupled predation--conversion event. Absorbing coordinate axes are imposed to represent the irreversibility of demographic extinction. Under Kurtz density-dependent scaling, the law-of-large-numbers limit recovers the classical RM ODE, while central-limit scaling yields a chemical-Langevin diffusion with explicit drift and full state-dependent covariance. A distinctive signature is the strictly negative cross-covariance $\u03a3_{12}(N,P)=-mNP/(1+N)$ induced solely by the predation--conversion increment $(-1,1)$. We define the absorbed It\u00f4 SDE by freezing trajectories at the first boundary hit and prove strong well-posedness, non-explosion, and moment bounds up to absorption. Extinction has positive probability from every interior state, and predator extinction is almost sure when $m\\le c$.",
      "domain": "math",
      "score": 7
    },
    {
      "paper_id": 747,
      "title": "Towards uncertainty quantification of a model for cancer-on-chip experiments",
      "abstract": "This study is a first step towards using data-informed differential models to predict and control the dynamics of cancer-on-chip experiments. We consider a conceptualized one-dimensional device, containing a cancer and a population of white blood cells. The interaction between the cancer and the population of cells is modeled by a chemotaxis model inspired by Keller-Segel-type equations, which is solved by a Hybridized Discontinuous Galerkin method. Our goal is using (synthetic) data to tune the parameters of the governing equations and to assess the uncertainty on the predictions of the dynamics due to the residual uncertainty on the parameters remaining after the tuning procedure. To this end, we apply techniques from uncertainty quantification for parametric differential models. We first perform a global sensitivity analysis using both Sobol and Morris indices to assess how parameter uncertainty impacts model predictions, and fix the value of parameters with negligible impact. Subsequently, we conduct an inverse uncertainty quantification analysis by Bayesian techniques to compute a data-informed probability distribution of the remaining model parameters. Finally, we carry out a forward uncertainty quantification analysis to compute the impact of the updated (residual) parametric uncertainties on the quantities of interest of the model. The whole procedure is sped up by using surrogate models, based on sparse-grids, to approximate the mapping of the uncertain parameters to the quantities of interest.",
      "domain": "math",
      "score": 7
    },
    {
      "paper_id": 753,
      "title": "Numerical stationary states for nonlocal Fokker-Planck equations via fixed points of consistency maps",
      "abstract": "We propose a fixed-point-based numerical framework for computing stationary states of nonlocal Fokker-Planck-type equations. Instead of discretising the differential operators directly, we reformulate the stationary problem as a nonlinear fixed-point map built from the original PDE and its nonlocal interaction terms, and solve the resulting finite-dimensional problem with a matrix-free Newton-Krylov method. We compare implementations using the analytic Frechet derivative of this map with a simple central-difference approximation. Because the method does not rely on time evolution, it is agnostic to dynamical stability and can detect both stable and unstable stationary states. Its accuracy is determined mainly by the numerical treatment of convolutions and quadrature, rather than by differentiation stencils. We apply the approach to three model problems with linear diffusion, use existing analytical results to verify the outputs, and reproduce known bifurcation diagrams, as well as new bifurcation behaviour not previously observed in this kind of problem.",
      "domain": "math",
      "score": 7
    },
    {
      "paper_id": 773,
      "title": "RAG without Forgetting: Continual Query-Infused Key Memory",
      "abstract": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 775,
      "title": "Thermodynamic Origin of Degree-Day Scaling in Phase-Change Systems",
      "abstract": "Phase transitions impose topological constraints on thermodynamic state variables, masking energetic fluctuations at the phase boundary. This constraint is most apparent in melting systems, where temperature remains pinned despite continued energy input. Here we resolve this information loss by introducing a latent temperature-a counterfactual trajectory describing the system's unconstrained thermal evolution. We show that energy conservation alone enforces a rigorous duality between the total latent heat dissipated during phase change and the accumulated exceedance of the latent temperature above the melting point. This duality is mathematically equivalent to the one-dimensional Wasserstein-1 distance between the latent and observed temperature trajectories, with the transport cost set by a characteristic surface dissipation timescale and melting energy. Applied to ice-sheet surface melting, this timescale admits a direct physical interpretation in terms of radiative and turbulent heat loss. The same framework yields a first-principles derivation of the empirical Positive Degree Day law and predicts realistic degree-day factors that emerge from surface energy balance, without ad hoc calibration. More broadly, phase change emerges as an optimal transport process that projects continuous energetic variability onto a constrained thermodynamic boundary.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 782,
      "title": "WADEPre: A Wavelet-based Decomposition Model for Extreme Precipitation Nowcasting with Multi-Scale Learning",
      "abstract": "The heavy-tailed nature of precipitation intensity impedes precise precipitation nowcasting. Standard models that optimize pixel-wise losses are prone to regression-to-the-mean bias, which blurs extreme values. Existing Fourier-based methods also lack the spatial localization needed to resolve transient convective cells. To overcome these intrinsic limitations, we propose WADEPre, a wavelet-based decomposition model for extreme precipitation that transitions the modeling into the wavelet domain. By leveraging the Discrete Wavelet Transform for explicit decomposition, WADEPre employs a dual-branch architecture: an Approximation Network to model stable, low-frequency advection, isolating deterministic trends from statistical bias, and a spatially localized Detail Network to capture high-frequency stochastic convection, resolving transient singularities and preserving sharp boundaries. A subsequent Refiner module then dynamically reconstructs these decoupled multi-scale components into the final high-fidelity forecast. To address optimization instability, we introduce a multi-scale curriculum learning strategy that progressively shifts supervision from coarse scales to fine-grained details. Extensive experiments on the SEVIR and Shanghai Radar datasets demonstrate that WADEPre achieves state-of-the-art performance, yielding significant improvements in capturing extreme thresholds and maintaining structural fidelity. Our code is available at https://github.com/sonderlau/WADEPre.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 783,
      "title": "Role of the ocean for fast atmospheric evolution revealed by machine learning",
      "abstract": "There have recently been many efforts to create machine learnt atmospheric emulators designed to replace physical models. So far these have mainly focused on medium-range weather forecasting, where these `Machine Learnt Weather Prediction' (MLWP) models can outperform leading operational forecasting centres. However, because of this focus on shorter timescales, many of these emulators ignore the effects of the ocean, and take no ocean variables as inputs. We hypothesise that such MLWP models have learnt a best-guess of the evolution of the atmosphere, by implicitly inferring ocean conditions from atmospheric states, with no access to ocean data. Turning this limitation into a strength, we use it as a means to study the role of the oceans on the evolution of the atmosphere. By exploring how model forecast errors relate to properties of the air-sea interface, we infer what ocean information these atmospheric emulators are able to derive from atmospheric data alone, and what they cannot. This highlights the regions and processes through which the ocean independently influences the atmosphere on fast timescales. We perform this analysis for GraphCast, finding clear relationships between air-sea properties and the forecast errors over the ocean, including clear seasonal effects. We then explore what this reveals about GraphCast's internal representation of the ocean. In addition to understanding real-world ocean-atmosphere interactions, this analysis provides guidance for improving forecast skill and physical realism in MLWP models, and for informing how future machine learning models should use ocean information on short timescales.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 785,
      "title": "Radar-Based Raindrop Size Distribution Prediction: Comparing Analytical, Neural Network, and Decision Tree Approaches",
      "abstract": "Reliable estimation of the raindrop size distribution (RSD) is important for applications including quantitative precipitation estimation, soil erosion modelling, and wind turbine blade erosion. While in situ instruments such as disdrometers provide detailed RSD measurements, they are spatially limited, motivating the use of polarimetric radar for remote retrieval of rain microphysical properties. This study presents a comparative evaluation of analytical and machine-learning approaches for retrieving RSD parameters from polarimetric radar observables. One-minute OTT Parsivel2 disdrometer measurements collected between September 2020 and May 2022 at Sheepdrove Farm, UK, were quality-controlled using collocated weighing and tipping-bucket rain gauges. Measured RSDs were fitted to a normalised three-parameter gamma distribution, from which a range of polarimetric radar variables were analytically simulated. Analytical retrievals, neural networks, and decision tree models were then trained to estimate the gamma distribution parameters across multiple radar feature sets and model architectures. To assess robustness and equifinality, each model configuration was trained 100 times using random 70/30 train-test splits, yielding approximately 17,000 trained models in total. Machine-learning approaches generally outperform analytical methods; however, no single model class or architecture is uniformly optimal. Model performance depends strongly on both the target RSD parameter and the available radar observables, with decision trees showing particular robustness in reduced-feature regimes. These results highlight the importance of aligning retrieval model structure with operational data constraints rather than adopting a single universal approach.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 787,
      "title": "HybridOM: Hybrid Physics-Based and Data-Driven Global Ocean Modeling with Efficient Spatial Downscaling",
      "abstract": "Global ocean modeling is vital for climate science but struggles to balance computational efficiency with accuracy. Traditional numerical solvers are accurate but computationally expensive, while pure deep learning approaches, though fast, often lack physical consistency and long-term stability. To address this, we introduce HybridOM, a framework integrating a lightweight, differentiable numerical solver as a skeleton to enforce physical laws, with a neural network as the flesh to correct subgrid-scale dynamics. To enable efficient high-resolution modeling, we further introduce a physics-informed regional downscaling mechanism based on flux gating. This design achieves the inference efficiency of AI-based methods while preserving the accuracy and robustness of physical models. Extensive experiments on the GLORYS12V1 and OceanBench dataset validate HybridOM's performance in two distinct regimes: long-term subseasonal-to-seasonal simulation and short-term operational forecasting coupled with the FuXi-2.0 weather model. Results demonstrate that HybridOM achieves state-of-the-art accuracy while strictly maintaining physical consistency, offering a robust solution for next-generation ocean digital twins. Our source code is available at https://github.com/ChiyodaMomo01/HybridOM.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 794,
      "title": "Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data",
      "abstract": "The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 802,
      "title": "Comment on \"Repair of DNA Double-Strand Breaks Leaves Heritable Impairment to Genome Function\"",
      "abstract": "Bantele and colleagues recently reported that repair of a single CRISPR/Cas9-induced DNA double-strand break (DSB) in the c-MYC topologically associated domain leads to a persistent depletion of chromatin interactions and long-term transcriptional attenuation across multiple generations of human cells. They interpret this observation as evidence for a previously unrecognized principle--\"chromatin fatigue\"--in which DSB repair generates a stable architectural defect that acts as a heritable impairment to genome function. Such an idea, if correct, would carry profound implications for genome biology, epigenetic inheritance, cancer evolution, aging, and the safety of therapeutic genome editing. However, our detailed reassessment of the experimental design, underlying assumptions, and data interpretation reveals that the evidence provided is inadequate to support these sweeping conclusions. Instead, the observed outcomes are more plausibly explained by a combination of Cas9 persistence, off-target DNA damage, repair-factor retention, MYC enhancer plasticity, and the well-documented genomic instability of HeLa cells. The study does not demonstrate mechanistic causality, does not exclude simpler explanations, and does not provide data consistent with true chromatin memory or heritable architectural change. Moreover, its statistical inferences are based on noisy measurements that fall within expected variability of unstable oncogenic loci. Here, we present a comprehensive critical analysis showing that the proposed model of chromatin fatigue is unsupported by the available evidence. We offer a corrected interpretation in which the chromatin landscape experiences a temporary, repair-associated perturbation that resolves without leaving enduring or heritable impairment.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 843,
      "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models",
      "abstract": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 847,
      "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction",
      "abstract": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 855,
      "title": "EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering",
      "abstract": "Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 871,
      "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
      "abstract": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
      "domain": "q-fin",
      "score": 7
    },
    {
      "paper_id": 907,
      "title": "Decaying Sensitivity of the Zero Solution for a Class of Nonlinear Optimal Control Problems",
      "abstract": "We study spatial decay properties of sensitivities in a nonlinear optimal control problem with a graph--structured interaction topology. For a problem with nonlinear decoupled dynamics and quadratic cost, we show that a localized perturbation of the zero reference leads to an optimal trajectory that decays exponentially with the graph distance. The analysis, based on a nonlinear controllability condition, provides a first step toward extending known spatial decay results from linear--quadratic to nonlinear systems. A numerical example illustrates the theoretical findings.",
      "domain": "math",
      "score": 7
    },
    {
      "paper_id": 925,
      "title": "Integrating prior knowledge in equation discovery: Interpretable symmetry-informed neural networks and symbolic regression via characteristic curves",
      "abstract": "Data-driven equation discovery aims to reconstruct governing equations directly from empirical observations. A fundamental challenge in this domain is the ill-posed nature of the inverse problem, where multiple distinct mathematical models may yield similar errors, thus complicating model selection and failing to guarantee a unique representation of the underlying mechanisms. This issue can be addressed by incorporating inductive biases to constrain the search space and discard the undesirable models. The characteristic curves-based (CCs) framework offers a modular approach ideally suited to this aim. This approach is based on the specification of structural families that possess provable identifiability properties. Crucially, this framework enables practitioners to embed domain expertise directly into the learning process and facilitates the integration of diverse post-processing tools. In this work, we build upon the recent neural network implementation of this formalism (NN-CC), which benefits from the universal approximation capabilities of NNs. Specifically, we extend NN-CC by introducing two inductive biases: (i) symmetry constraints and (ii) post-processing with symbolic regression. Using a chaotic Duffing oscillator and a discontinuous stick-slip model under varying Gaussian noise levels, we show how these extensions systematically improve the discovery process. We also analyze the integration of sparse and symbolic regression (using SINDy and PySR) into the CC-based formalism. These extensions (SINDy-CC and SR-CC) consistently show improvements as prior information is incorporated. By enabling the integration of prior or hypothesized knowledge into the learning and post-processing stages, the CC-based formalism emerges as a promising candidate to address identifiability issues in purely data-driven methods, advancing the goal of interpretable and reliable system identification.",
      "domain": "nlin",
      "score": 7
    },
    {
      "paper_id": 937,
      "title": "GenAI-Net: A Generative AI Framework for Automated Biomolecular Network Design",
      "abstract": "Biomolecular networks underpin emerging technologies in synthetic biology-from robust biomanufacturing and metabolic engineering to smart therapeutics and cell-based diagnostics-and also provide a mechanistic language for understanding complex dynamics in natural and ecological systems. Yet designing chemical reaction networks (CRNs) that implement a desired dynamical function remains largely manual: while a proposed network can be checked by simulation, the reverse problem of discovering a network from a behavioral specification is difficult, requiring substantial human insight to navigate a vast space of topologies and kinetic parameters with nonlinear and possibly stochastic dynamics. Here we introduce GenAI-Net, a generative AI framework that automates CRN design by coupling an agent that proposes reactions to simulation-based evaluation defined by a user-specified objective. GenAI-Net efficiently produces novel, topologically diverse solutions across multiple design tasks, including dose responses, complex logic gates, classifiers, oscillators, and robust perfect adaptation in deterministic and stochastic settings (including noise reduction). By turning specifications into families of circuit candidates and reusable motifs, GenAI-Net provides a general route to programmable biomolecular circuit design and accelerates the translation from desired function to implementable mechanisms.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 939,
      "title": "A generalized work theorem for stopped stochastic chemical reaction networks",
      "abstract": "We establish a generalized work theorem for stochastic chemical reaction networks (CRNs). By using a compensated Poisson jump process, we identify a martingale structure in a generalized entropy defined relative to an auxiliary backward process and extend nonequilibrium work relations to processes stopped at bounded arbitrary times. Our results apply to discrete, mesoscopic chemical reaction networks and remain valid for singular initial conditions and state-dependent termination events. We show how martingale properties emerge directly from the structure of reaction propensities without assuming detailed balance. Stochastic simulations of a simple chemical kinetic proofreading network are used to explore the dependence of the exponentiated entropy production on initial conditions and model parameters, validating our new work theorem relationships. Our results provide new quantitative tools for analyzing biological circuits ranging from metabolic to gene regulation pathways.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 944,
      "title": "Crossing the Functional Desert: Cascade-Driven Assembly and Feasibility Transitions in Early Life",
      "abstract": "The origin of life poses a problem of combinatorial feasibility: How can temporally supported functional organization arise in exponentially branching assembly spaces when unguided exploration behaves as a memoryless random walk? We show that nonlinear threshold-cascade dynamics in connected interaction networks provide a minimal, substrate-agnostic mechanism that can soften this obstruction. Below a critical connectivity threshold, cascades die out locally and structured input-output response mappings remain sparse and transient-a \"functional desert\" in which accumulation is dynamically unsupported. Near the critical percolation threshold, system-spanning cascades emerge, enabling discriminative functional responses. We illustrate this transition using a minimal toy model and generalize the argument to arbitrary networked systems. Also near criticality, cascades introduce finite-timescale structural and functional coherence, directional bias, and weak dynamical path-dependence into otherwise memoryless exploration, allowing biased accumulation. This connectivity-driven transition-functional percolation-requires only generic ingredients: interacting units, nonlinear thresholds, influence transmission, and non-zero coherence times. The mechanism does not explain specific biochemical pathways, but it identifies a necessary dynamical regime in which structured functional organization can emerge and be temporarily supported, providing a physical foundation for how combinatorial feasibility barriers can be crossed through network dynamics alone.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 970,
      "title": "Orthogonal Model Merging",
      "abstract": "Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 990,
      "title": "Understanding the temperature response of biological systems: Part II -- Network-level mechanisms and emergent dynamics",
      "abstract": "Building on the phenomenological and microscopic models reviewed in Part I, this second part focuses on network-level mechanisms that generate emergent temperature response curves. We review deterministic models in which temperature modulates the kinetics of coupled biochemical reactions, as well as stochastic frameworks, such as Markov chains, that capture more complex multi-step processes. These approaches show how Arrhenius-like temperature dependence at the level of individual reactions is transformed into non-Arrhenius scaling, thermal limits, and temperature compensation at the system level. Together, network-level models provide a mechanistic bridge between empirical temperature response curves and the molecular organization of biological systems, giving us predictive insights into robustness, perturbations, and evolutionary constraints.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 996,
      "title": "Causal Online Learning of Safe Regions in Cloud Radio Access Networks",
      "abstract": "Cloud radio access networks (RANs) enable cost-effective management of mobile networks by dynamically scaling their capacity on demand. However, deploying adaptive controllers to implement such dynamic scaling in operational networks is challenging due to the risk of breaching service agreements and operational constraints. To mitigate this challenge, we present a novel method for learning the safe operating region of the RAN, i.e., the set of resource allocations and network configurations for which its specification is fulfilled. The method, which we call (C)ausal (O)nline (L)earning, operates in two online phases: an inference phase and an intervention phase. In the first phase, we passively observe the RAN to infer an initial safe region via causal inference and Gaussian process regression. In the second phase, we gradually expand this region through interventional Bayesian learning. We prove that COL ensures that the learned region is safe with a specified probability and that it converges to the full safe region under standard conditions. We experimentally validate COL on a 5G testbed. The results show that COL quickly learns the safe region while incurring low operational cost and being up to 10x more sample-efficient than current state-of-the-art methods for safe learning.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 998,
      "title": "LLM-Empowered Cooperative Content Caching in Vehicular Fog Caching-Assisted Platoon Networks",
      "abstract": "This letter proposes a novel three-tier content caching architecture for Vehicular Fog Caching (VFC)-assisted platoon, where the VFC is formed by the vehicles driving near the platoon. The system strategically coordinates storage across local platoon vehicles, dynamic VFC clusters, and cloud server (CS) to minimize content retrieval latency. To efficiently manage distributed storage, we integrate large language models (LLMs) for real-time and intelligent caching decisions. The proposed approach leverages LLMs' ability to process heterogeneous information, including user profiles, historical data, content characteristics, and dynamic system states. Through a designed prompting framework encoding task objectives and caching constraints, the LLMs formulate caching as a decision-making task, and our hierarchical deterministic caching mapping strategy enables adaptive requests prediction and precise content placement across three tiers without frequent retraining. Simulation results demonstrate the advantages of our proposed caching scheme.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1003,
      "title": "xDevSM: An Open-Source Framework for Portable, AI-Ready xApps Across Heterogeneous O-RAN Deployments",
      "abstract": "Openness and programmability in the O-RAN architecture enable closed-loop control of the Radio Access Network (RAN). Artificial Intelligence (AI)-driven xApps, in the near-real-time RAN Intelligent Controller (RIC), can learn from network data, anticipate future conditions, and dynamically adapt radio configurations. However, their development and adoption are hindered by the complexity of low-level RAN control and monitoring message models exposed over the O-RAN E2 interface, limited interoperability across heterogeneous RAN software stacks, and the lack of developer-friendly frameworks. In this paper, we introduce xDevSM, a framework that significantly lowers the barrier to xApp development by unifying observability and control in O-RAN deployment. By exposing a rich set of Key Performance Measurements (KPMs) and enabling fine-grained radio resource management controls, xDevSM provides the essential foundation for practical AI-driven xApps. We validate xDevSM on real-world testbeds, leveraging Commercial Off-the-Shelf (COTS) devices together with heterogeneous RAN hardware, including Universal Software Radio Peripheral (USRP)-based Software-defined Radios (SDRs) and Foxconn radio units, and show its seamless interoperability across multiple open-source RAN software stacks. Furthermore, we discuss and evaluate the capabilities of our framework through three O-RAN-based scenarios of high interest: (i) KPM-based monitoring of network performance, (ii) slice-level Physical Resource Block (PRB) allocation control across multiple User Equipments (UEs) and slices, and (iii) mobility-aware handover control, showing that xDevSM can implement intelligent closed-loop applications, laying the groundwork for learning-based optimization in heterogeneous RAN deployments. xDevSM is open source and available as foundational tool for the research community.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1005,
      "title": "Morphe: High-Fidelity Generative Video Streaming with Vision Foundation Model",
      "abstract": "Video streaming is a fundamental Internet service, while the quality still cannot be guaranteed especially in poor network conditions such as bandwidth-constrained and remote areas. Existing works mainly work towards two directions: traditional pixel-codec streaming nearly approaches its limit and is hard to step further in compression; the emerging neural-enhanced or generative streaming usually fall short in latency and visual fidelity, hindering their practical deployment. Inspired by the recent success of vision foundation model (VFM), we strive to harness the powerful video understanding and processing capacities of VFM to achieve generalization, high fidelity and loss resilience for real-time video streaming with even higher compression rate. We present the first revolutionized paradigm that enables VFM-based end-to-end generative video streaming towards this goal. Specifically, Morphe employs joint training of visual tokenizers and variable-resolution spatiotemporal optimization under simulated network constraints. Additionally, a robust streaming system is constructed that leverages intelligent packet dropping to resist real-world network perturbations. Extensive evaluation demonstrates that Morphe achieves comparable visual quality while saving 62.5\\% bandwidth compared to H.265, and accomplishes real-time, loss-resilient video delivery in challenging network environments, representing a milestone in VFM-enabled multimedia streaming solutions.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1023,
      "title": "Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs",
      "abstract": "In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain $D$ that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by~\\cite{RegisterAllocation}. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of \\(O(|G| \\cdot |D|^6)\\), where \\(|G|\\) represents the size of the control-flow graph. Note that for any fixed domain $D,$ this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1025,
      "title": "COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation",
      "abstract": "Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1046,
      "title": "Low-Dimensional Interaction Spaces Impose Geometric Constraints On Collective Organization",
      "abstract": "Collective organization in physical, biophysical, and biological systems often emerges from many weak, local interactions, yet the resulting global structures display striking regularities and apparent limits in diversity. Existing theoretical approaches typically emphasize specific mechanisms, detailed dynamics, or energetic optimization, making it difficult to identify constraints that are independent of microscopic realization. Here we develop a general theoretical framework showing that, when effective interactions among system components compress into a low-dimensional interaction space, global organization is governed by geometric constraints rather than detailed dynamics. We formalize interaction spaces as metric manifolds derived from coarse-grained effective couplings and show that low interaction dimensionality imposes upper bounds on the number, separability, and robustness of distinct collective organizations. These results yield impossibility statements: many conceivable macroscopic organizations are excluded a priori, even when locally compatible interactions exist. The framework applies across equilibrium and nonequilibrium systems without assuming specific symmetries or conservation laws. By shifting the explanatory focus from generative mechanisms to structural constraints, this work establishes a general, geometry-based perspective on collective organization.",
      "domain": "q-bio",
      "score": 7
    },
    {
      "paper_id": 1068,
      "title": "Accelerated Electromagnetic Simulation of MRI RF Interactions with Graphene Microtransistor-Based Neural Probes for Electrophysiology-fMRI Integration",
      "abstract": "Implementing electrophysiological recordings within an MRI environment is challenging due to complex interactions between recording probes and MRI-generated fields, which can affect both safety and data quality. This study aims to develop and evaluate a hybrid electromagnetic (EM) simulation framework for efficient and accurate assessment of such interactions. Methods: A hybrid EM strategy integrating the Huygens' Box (HB) method with sub-gridding was implemented in an FDTD solver (Sim4Life). RF coil models for mouse and rat head were simulated with and without intracortical (IC) and epicortical (EC) graphene-based micro-transistor arrays. Three-dimensional multi-layered probe models were reconstructed from two-dimensional layouts, and transmit field ($B_{1}^{+}$), electric field ($E$), and specific absorption rate (SAR) distributions were evaluated. Performance was benchmarked against conventional full-wave multi-port (MP) simulations using Bland-Altman analysis and voxel-wise percentage differences. Results: HB simulations reduced computational time by approximately 70-80%, while preserving spatial patterns of $|B_{1}^{+}|$, $|E|$, and SAR, including transmit-field symmetry and localized high-field regions. Deviations from MP were minimal for $|B_{1}^{+}|$ (median $\u0394$% 0.02-0.07% in mice, -3.7% to -1.7% in rats) and modest for $|E|$ and SAR, with absolute SAR values remaining well below human safety limits. Graphene-based arrays produced negligible effects on RF transmission and SAR deposition. Conclusion: The HB approach enables computationally efficient, high-resolution evaluation of EM interactions involving microscopic probes in MRI environments, supporting simulations that are otherwise impractical with full-wave MP modeling.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 1069,
      "title": "Multiparameter Uncertainty Mapping in Quantitative Molecular MRI using a Physics-Structured Variational Autoencoder (PS-VAE)",
      "abstract": "Quantitative imaging methods, such as magnetic resonance fingerprinting (MRF), aim to extract interpretable pathology biomarkers by estimating biophysical tissue parameters from signal evolutions. However, the pattern-matching algorithms or neural networks used in such inverse problems often lack principled uncertainty quantification, which limits the trustworthiness and transparency, required for clinical acceptance. Here, we describe a physics-structured variational autoencoder (PS-VAE) designed for rapid extraction of voxelwise multi-parameter posterior distributions. Our approach integrates a differentiable spin physics simulator with self-supervised learning, and provides a full covariance that captures the inter-parameter correlations of the latent biophysical space. The method was validated in a multi-proton pool chemical exchange saturation transfer (CEST) and semisolid magnetization transfer (MT) molecular MRF study, across in-vitro phantoms, tumor-bearing mice, healthy human volunteers, and a subject with glioblastoma. The resulting multi-parametric posteriors are in good agreement with those calculated using a brute-force Bayesian analysis, while providing an orders-of-magnitude acceleration in whole brain quantification. In addition, we demonstrate how monitoring the multi-parameter posterior dynamics across progressively acquired signals provides practical insights for protocol optimization and may facilitate real-time adaptive acquisition.",
      "domain": "physics",
      "score": 7
    },
    {
      "paper_id": 1100,
      "title": "Integrated Sensing, Communication, and Control for UAV-Assisted Mobile Target Tracking",
      "abstract": "Unmanned aerial vehicles (UAVs) are increasingly deployed in mission-critical applications such as target tracking, where they must simultaneously sense dynamic environments, ensure reliable communication, and achieve precise control. A key challenge here is to jointly guarantee tracking accuracy, communication reliability, and control stability within a unified framework. To address this issue, we propose an integrated sensing, communication, and control (ISCC) framework for UAV-assisted target tracking, where the considered tracking system is modeled as a discrete-time linear control process, with the objective of driving the deviation between the UAV and target states toward zero. We formulate a stochastic model predictive control (MPC) optimization problem for joint control and beamforming design, which is highly non-convex and intractable in its original form. To overcome this difficulty, the target state is first estimated using an extended Kalman filter (EKF). Then, by deriving the closed-form optimal beamforming solution under a given control input, the original problem is equivalently reformulated into a tractable control-oriented form. Finally, we convexify the remaining non-convex constraints via a relaxation-based convex approximation, yielding a computationally tractable convex optimization problem that admits efficient global solution. Numerical results show that the proposed ISCC framework achieves tracking accuracy comparable to a non-causal benchmark while maintaining stable communication, and it significantly outperforms the conventional control and tracking method.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1147,
      "title": "System-Level Isolation for Mixed-Criticality RISC-V SoCs: A \"World\" Reality Check",
      "abstract": "As RISC-V adoption accelerates, domains such as automotive, the Internet of Things (IoT), and industrial control are attracting growing attention. These domains are subject to stringent Size, Weight, Power, and Cost (SWaP-C) constraints, which have driven a shift toward heterogeneous Systems-on-Chip (SoCs) integrating general-purpose CPUs, tightly coupled accelerators, and diverse I/O devices with different integrity levels. While such integration improves cost efficiency and performance, it introduces a fundamental safety and security challenge: enforcing system-level isolation in mixed-criticality environments. Although RISC-V International has proposed several hardware isolation primitives, including RISC-V Worlds, IOPMP, and SmMTT, their interoperability, scalability, and suitability for real-time systems remain insufficiently understood. In this paper, we present a comparative analysis of these primitives from the perspective of practical heterogeneous SoC designs. We implement an IOPMP, a World-based checker, and a modified RISC-V World checker that addresses key limitations of the baseline specification, and evaluate their trade-offs in terms of security guarantees and power-performance-area (PPA). Our results show that the World-based checker introduces a fixed, configuration-independent access latency, achieving lower worst-case delay than the evaluated alternatives while scaling predictably with system size. At the macro level, we estimate that the proposed modifications reduce SoC area by up to approximately 5% compared to a baseline design. All artifacts will be released as open source, and we expect these findings to directly contribute to the evolution and ratification of RISC-V specifications, as well as to the design of future RISC-V SoCs.",
      "domain": "cs",
      "score": 7
    },
    {
      "paper_id": 1150,
      "title": "Crypto-RV: High-Efficiency FPGA-Based RISC-V Cryptographic Co-Processor for IoT Security",
      "abstract": "Cryptographic operations are critical for securing IoT, edge computing, and autonomous systems. However, current RISC-V platforms lack efficient hardware support for comprehensive cryptographic algorithm families and post-quantum cryptography. This paper presents Crypto-RV, a RISC-V co-processor architecture that unifies support for SHA-256, SHA-512, SM3, SHA3-256, SHAKE-128, SHAKE-256 AES-128, HARAKA-256, and HARAKA-512 within a single 64-bit datapath. Crypto-RV introduces three key architectural innovations: a high-bandwidth internal buffer (128x64-bit), cryptography-specialized execution units with four-stage pipelined datapaths, and a double-buffering mechanism with adaptive scheduling optimized for large-hash. Implemented on Xilinx ZCU102 FPGA at 160 MHz with 0.851 W dynamic power, Crypto-RV achieves 165 times to 1,061 times speedup over baseline RISC-V cores, 5.8 times to 17.4 times better energy efficiency compared to powerful CPUs. The design occupies only 34,704 LUTs, 37,329 FFs, and 22 BRAMs demonstrating viability for high-performance, energy-efficient cryptographic processing in resource-constrained IoT environments.",
      "domain": "cs",
      "score": 7
    }
  ]
}